{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_path = '/Users/sepehrbe/My_Drive/DataSources/Project_Data' # Please define the directory in which all the images and data will be stored\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from statistics import mean\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import pickle as pickle\n",
    "import PIL as pl\n",
    "import pandas as pd\n",
    "from PIL.Image import Transpose\n",
    "from PIL import Image\n",
    "from usefull_functions import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import AutoEncoders Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3833/3833 [00:05<00:00, 741.18it/s]\n",
      "100%|██████████| 407/407 [00:00<00:00, 776.51it/s]\n",
      "100%|██████████| 407/407 [00:00<00:00, 785.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def import_imges(img_path):\n",
    "    img_list = []\n",
    "    for path, dirs, files in  os.walk(img_path):\n",
    "        for f in files:\n",
    "            img_list.append( path +'/' + f)\n",
    "        for d in dirs:\n",
    "            img_list.append( path + d)\n",
    "    img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "    dataset = pd.DataFrame(columns=['image_id','img_array'])\n",
    "    for img in tqdm(img_list):\n",
    "        img_name = re.findall('\\w+', img)[-2:-1][0]\n",
    "        image = pl.Image.open( img)\n",
    "        img_array = np.asarray(image)\n",
    "        new_row = pd.Series({'image_id':img_name,'img_array':img_array}, name='')\n",
    "        dataset = dataset.append(new_row)\n",
    "    return dataset\n",
    "\n",
    "train_path = G_path + '/03_AutoEncoded_Images/train'\n",
    "valid_path = G_path + '/03_AutoEncoded_Images/valid'\n",
    "test_path = G_path + '/03_AutoEncoded_Images/test'\n",
    "\n",
    "train_set = import_imges(train_path)\n",
    "valid_set = import_imges(valid_path)\n",
    "test_set = import_imges(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_Data_Augmented = pickle.load(open(G_path + '/02_Augmented_MetaData/'+ '/Meta_Data_Augmented','rb'))\n",
    "blc_test_meta = pickle.load(open(G_path + '/02_Augmented_MetaData/'+ '/balanced_test_set','rb'))\n",
    "dataset_ = pd.concat([train_set,valid_set,test_set],axis=0)\n",
    "A_E_dataset = dataset_.merge(Meta_Data_Augmented, how='left', on='image_id')\n",
    "blc_test_dataset_A_E = dataset_[['image_id','img_array']].merge(blc_test_meta, how=\"right\",on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3833  Valid Size:  407  Test size:  407\n"
     ]
    }
   ],
   "source": [
    "df_ = A_E_dataset\n",
    "train_data = df_[df_.type=='train']\n",
    "valid_data = df_[df_.type=='valid']\n",
    "test_data = df_[df_.type=='test']\n",
    "print('Train size: ',train_data.shape[0] ,' Valid Size: ',valid_data.shape[0], ' Test size: ', test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.36115692 0.34408239 0.32405245]   STD: [0.16846211 0.26209011 0.24330585]\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.concat([train_data,valid_data], axis=0)\n",
    "training_arrays_scaled = np.stack(training_data['img_array'].values)  / 255\n",
    "Mean = training_arrays_scaled.mean(axis = (0,1,2)) \n",
    "STD = training_arrays_scaled.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean}   STD: {STD}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data standardization\n",
    "train_arrays_std = np.stack(train_data['img_array'].values) / 255\n",
    "for i in range(0,train_arrays_std.shape[0]):\n",
    "    train_arrays_std[i] = (train_arrays_std[i] - Mean) / STD\n",
    "#Validation data standardization\n",
    "valid_arrays_std = np.stack(valid_data['img_array'].values) / 255\n",
    "for i in range(0,valid_arrays_std.shape[0]):\n",
    "    valid_arrays_std[i] = (valid_arrays_std[i] - Mean) / STD\n",
    "#Test data standardization\n",
    "test_arrays_std = np.stack(test_data['img_array'].values) / 255\n",
    "for i in range(0,test_arrays_std.shape[0]):\n",
    "    test_arrays_std[i] = (test_arrays_std[i] - Mean) / STD\n",
    "\n",
    "#Test-Valid data standardization\n",
    "test_valid_arrays_std = np.stack(blc_test_dataset_A_E['img_array'].values) / 255\n",
    "for i in range(0,test_valid_arrays_std.shape[0]):\n",
    "    test_valid_arrays_std[i] = (test_valid_arrays_std[i] - Mean) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean : [-0.04389305 -0.05144053  0.03501551]   Test Set STD: [0.93557654 0.98728975 1.00074642]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = test_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = test_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Test Set Mean : {Mean_t}   Test Set STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metda_keys = train_data[['type','image_id']].reset_index(drop=True), valid_data[['type','image_id']].reset_index(drop=True), test_data[['type','image_id']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays_std_T = torch.tensor(train_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "train_labels = torch.tensor(train_data['image_label'].values.astype(np.long))\n",
    "\n",
    "valid_arrays_std_T = torch.tensor(valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "valid_labels = torch.tensor(valid_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_valid_arrays_std_T = torch.tensor(test_valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_valid_labels = torch.tensor(blc_test_dataset_A_E['image_label'].values.astype(np.long))\n",
    "\n",
    "A_E_input_data = (train_arrays_std_T , valid_arrays_std_T, test_arrays_std_T ,metda_keys)\n",
    "\n",
    "train_labels_binary =  torch.tensor(label_to_binary(train_labels))\n",
    "valid_labels_binary =  torch.tensor(label_to_binary(valid_labels))\n",
    "test_labels_binary =  torch.tensor(label_to_binary(test_labels))\n",
    "\n",
    "A_E_labels = ((train_labels,valid_labels,test_labels), (train_labels_binary , valid_labels_binary ,test_labels_binary))\n",
    "\n",
    "A_E_balc_test_set = (test_valid_arrays_std_T,test_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((A_E_input_data,A_E_labels), open(G_path + '/05_AutoEncoded_DataSet/'+ 'Input_DataSet_A_E', 'wb'))\n",
    "pickle.dump(A_E_balc_test_set, open(G_path +  '/05_AutoEncoded_DataSet/'+  '/Balanced_Test_Set_A_E', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
