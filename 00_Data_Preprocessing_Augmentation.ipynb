{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features.usefull_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__vsc_ipynb_file__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'grid_searc_cross_valid_trainer',\n",
       " 'json',\n",
       " 'label_to_binary',\n",
       " 'mean',\n",
       " 'model_evaluation',\n",
       " 'np',\n",
       " 'os',\n",
       " 'pd',\n",
       " 'plot_grid_results',\n",
       " 'plot_loss_accuracy',\n",
       " 'plt',\n",
       " 'progressbar',\n",
       " 'quit',\n",
       " 'recall_specificity_precision',\n",
       " 'remap',\n",
       " 'sk',\n",
       " 'sns',\n",
       " 'sys',\n",
       " 'tqdm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_path = '/Users/sepehrbe/My_Drive/DataSources/Project_Data' # Please define the directory in which all the images and data will be stored\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from statistics import mean\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import pickle as pickle\n",
    "import PIL as pl\n",
    "import pandas as pd\n",
    "from PIL.Image import Transpose\n",
    "from PIL import Image\n",
    "from usefull_functions import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Meta Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'HAM10000_metadata.csv'\n",
    "meta_data_v0 = pd.read_csv(G_path + '/00_HAM1000_DataSet/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplication\n",
    "meta_data_noDup = meta_data_v0.groupby('lesion_id').first().reset_index()\n",
    "# Encode Labels\n",
    "meta_data_noDup.dx = pd.Categorical(meta_data_noDup.dx)\n",
    "meta_data_noDup['image_label'] = meta_data_noDup.dx.cat.codes\n",
    "\n",
    "remap = {0:1 , 1:1, 4:1, 2:0, 3:0, 5:0, 6:0}\n",
    "meta_data_noDup['binary_label'] = meta_data_noDup['image_label']\n",
    "meta_data_noDup = meta_data_noDup.replace({'binary_label':remap})\n",
    "\n",
    "meta_data_noDup.to_csv(G_path + '/02_Augmented_MetaData/' + 'Meta_data_noDup.csv')\n",
    "### insert the code of R for Age filling\n",
    "\n",
    "######################\n",
    "filled_nan_ages = pd.read_csv(G_path + '/02_Augmented_MetaData/'+'/no_duplicates_no_NAs.csv').rename(columns={'age':'age_mdf'})\n",
    "\n",
    "meta_data_noDup = meta_data_noDup.merge(filled_nan_ages,how='left', on='lesion_id')\n",
    "meta_data_noDup['age'] = meta_data_noDup.apply(lambda x: x.age if pd.notnull(x.age) else x.age_mdf, axis=1)\n",
    "\n",
    "try: meta_data_noDup.drop(columns=['Unnamed: 0','age_mdf'],inplace=True)\n",
    "except: None\n",
    "# Reduce Nr. class 5 category to 2800\n",
    "class_5 = meta_data_noDup[meta_data_noDup.image_label==5]\n",
    "class_5_reduced = class_5.sample(n=2000, random_state=0)\n",
    "meta_data_noDup_reduced = pd.concat([meta_data_noDup[meta_data_noDup.image_label!=5],class_5_reduced],axis=0)\n",
    "# Get \n",
    "train_meta, test_valid_meta = train_test_split(meta_data_noDup_reduced, test_size = 0.2, random_state=0,stratify=meta_data_noDup_reduced['image_label'])\n",
    "test_meta, valid_meta = train_test_split(test_valid_meta, test_size = 0.5, random_state=0,stratify=test_valid_meta['image_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.723293\n",
       "2    0.097323\n",
       "4    0.082195\n",
       "1    0.043775\n",
       "0    0.030522\n",
       "6    0.013119\n",
       "3    0.009772\n",
       "Name: image_label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data_noDup.image_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta['type'] = 'train'\n",
    "valid_meta['type'] = 'valid'\n",
    "test_meta['type'] = 'test'\n",
    "blc_test_meta = pd.concat([valid_meta,test_meta],axis=0)\n",
    "# blc_test_meta = blc_test_meta.groupby('image_label').head(15).reset_index()\n",
    "blc_test_meta = blc_test_meta.groupby('image_label').sample(n=15,random_state=0)\n",
    "pickle.dump(blc_test_meta , open(G_path + '/02_Augmented_MetaData/'+ '/balanced_test_set','wb'))\n",
    "blc_test_meta.to_csv(G_path+'/02_Augmented_MetaData/' +'balanced_test_set.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "for path, dirs, files in  os.walk(G_path + '/00_HAM1000_DataSet'):\n",
    "    for f in files:\n",
    "        img_list.append( path +'/' + f)\n",
    "    for d in dirs:\n",
    "        img_list.append( path + d)\n",
    "img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Images for Augmentation Directory\n",
    "Augment_Path = G_path + '/' + '01_Augmented_Imaged'\n",
    "try:\n",
    "    os.makedirs(Augment_Path)\n",
    "except:\n",
    "    None\n",
    "try:\n",
    "    os.makedirs(Augment_Path+'/test')\n",
    "    os.makedirs(Augment_Path+'/valid')\n",
    "    os.makedirs(Augment_Path+'/train')\n",
    "except:\n",
    "    None\n",
    "labels = list(train_meta.image_label.unique())\n",
    "\n",
    "for i in labels:\n",
    "    try:\n",
    "        os.makedirs(Augment_Path+'/test/'+str(i))\n",
    "        os.makedirs(Augment_Path+'/valid/'+str(i))\n",
    "        os.makedirs(Augment_Path+'/train/'+str(i))\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = train_meta[['image_id','image_label']]\n",
    "name = 'train'\n",
    "def copy_to_AutoEncoder(data_ , name):\n",
    "    id_list = list(data_['image_id'])\n",
    "    id_labels = list(data_['image_label'])\n",
    "    for i in range(len(id_list)):\n",
    "        for j in img_list:\n",
    "            if id_list[i] in j:\n",
    "                shutil.copy(j, Augment_Path+'/' + name+ '/' + str(id_labels[i]) + '/' + id_list[i] + '.jpg')\n",
    "                break\n",
    "\n",
    "data_ = train_meta[['image_id','image_label']]\n",
    "name = 'train'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)\n",
    "data_ = valid_meta[['image_id','image_label']]\n",
    "name = 'valid'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)\n",
    "data_ = test_meta[['image_id','image_label']]\n",
    "name = 'test'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = Augment_Path + '/train/'\n",
    "\n",
    "for i in [3, 6, 0, 1]:\n",
    "    f = folder_dir + str(i) + \"/\"\n",
    "    for image in os.listdir(f):\n",
    "        if \"ISIC\" in image and \"AUG_\" not in image and image.replace(\".jpg\", \"\"):\n",
    "            # print(\"Augmented image: \", image)\n",
    "            im1 = Image.open(f + image)\n",
    "            augmented = im1.transpose(Transpose.FLIP_LEFT_RIGHT)\n",
    "            augmented = augmented.transpose(Transpose.FLIP_TOP_BOTTOM)\n",
    "            augmented.save(folder_dir + \"/\" + str(i) + \"/AUG_\" + image)\n",
    "            # Add metadata of Augmented image\n",
    "            new_row = train_meta[train_meta.image_id == image.replace(\".jpg\", \"\")]\n",
    "            new_row['image_id'] = 'AUG_' + new_row['image_id'].iloc[0]\n",
    "            train_meta = train_meta.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_Data_Augmented = pd.concat([train_meta, valid_meta, test_meta], axis=0)\n",
    "pickle.dump(Meta_Data_Augmented , open(G_path + '/02_Augmented_MetaData/'+ '/Meta_Data_Augmented','wb'))\n",
    "Meta_Data_Augmented.to_csv(G_path+'/02_Augmented_MetaData/' +'Meta_Data_Augmented.csv' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
