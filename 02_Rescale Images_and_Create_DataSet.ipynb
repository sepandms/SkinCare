{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  CWD = '/content/drive/MyDrive/DataSources/SkinCare'\n",
    "  # G_path = '/content/drive/MyDrive/DataSources/SkinCare_Project/SkinCare_Project/Project_Data'\n",
    "  os.chdir(CWD)\n",
    "except:None\n",
    "G_path = './Project_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from statistics import mean\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import pickle as pickle\n",
    "import PIL as pl\n",
    "import pandas as pd\n",
    "from PIL.Image import Transpose\n",
    "from PIL import Image\n",
    "from features import usefull_functions, CNN_NETs, MLP_Nets\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Augmented Images and rescaling to 280x210**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4647/4647 [01:37<00:00, 47.79it/s]\n"
     ]
    }
   ],
   "source": [
    "Augment_Path = Augment_Path = G_path + '/' + '01_Augmented_Imaged'\n",
    "img_list = []\n",
    "for path, dirs, files in  os.walk(Augment_Path):\n",
    "    for f in files:\n",
    "        img_list.append( path +'/' + f)\n",
    "    for d in dirs:\n",
    "        img_list.append( path + d)\n",
    "img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "Images_280x210 = G_path + '/04_Rescaled_Images'\n",
    "\n",
    "try:\n",
    "    os.makedirs(Images_280x210)\n",
    "except:\n",
    "    None\n",
    "\n",
    "for img in tqdm(img_list):\n",
    "    file_out = Images_280x210 +'/'+ re.findall('\\w+',img)[-2:-1][0] +'.jpg'\n",
    "    os.system('convert %s %s %s' % ('-resize 46.66%',img,file_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Rescaled Images and convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [00:09<00:00, 486.32it/s]\n"
     ]
    }
   ],
   "source": [
    "img_list = os.listdir(Images_280x210)\n",
    "Augmented_280x210_dataset = pd.DataFrame(columns=['image_id','img_array'])\n",
    "for img in tqdm(img_list):\n",
    "    if '.jpg' in img:\n",
    "        img_name = img.replace('.jpg','')\n",
    "        image = pl.Image.open( Images_280x210 +'/'+ img)\n",
    "        img_array = np.asarray(image)\n",
    "        new_row = pd.Series({'image_id':img_name,'img_array':img_array}, name='')\n",
    "        Augmented_280x210_dataset = Augmented_280x210_dataset.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_Data_Augmented = pickle.load(open(G_path + '/02_Augmented_MetaData/'+ '/Meta_Data_Augmented','rb'))\n",
    "blc_test_meta = pickle.load(open(G_path + '/02_Augmented_MetaData/'+ '/balanced_test_set','rb'))\n",
    "\n",
    "CNN_280x210_dataset = Augmented_280x210_dataset.merge(Meta_Data_Augmented, how=\"left\",on='image_id')\n",
    "blc_test_dataset_280x210 = Augmented_280x210_dataset[['image_id','img_array']].merge(blc_test_meta, how=\"right\",on='image_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN_280x210: Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3833  Valid Size:  407  Test size:  407\n"
     ]
    }
   ],
   "source": [
    "df_ = CNN_280x210_dataset\n",
    "train_data = df_[df_.type=='train']\n",
    "valid_data = df_[df_.type=='valid']\n",
    "test_data = df_[df_.type=='test']\n",
    "print('Train size: ',train_data.shape[0] ,' Valid Size: ',valid_data.shape[0], ' Test size: ', test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.76570269 0.55224556 0.57657015]   STD: [0.13616887 0.1481389  0.16343116]\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.concat([train_data,valid_data], axis=0)\n",
    "training_arrays_scaled = np.stack(training_data['img_array'].values)  / 255\n",
    "Mean = training_arrays_scaled.mean(axis = (0,1,2)) \n",
    "STD = training_arrays_scaled.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean}   STD: {STD}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data standardization\n",
    "train_arrays_std = np.stack(train_data['img_array'].values) / 255\n",
    "for i in range(0,train_arrays_std.shape[0]):\n",
    "    train_arrays_std[i] = (train_arrays_std[i] - Mean) / STD\n",
    "#Validation data standardization\n",
    "valid_arrays_std = np.stack(valid_data['img_array'].values) / 255\n",
    "for i in range(0,valid_arrays_std.shape[0]):\n",
    "    valid_arrays_std[i] = (valid_arrays_std[i] - Mean) / STD\n",
    "#Test data standardization\n",
    "test_arrays_std = np.stack(test_data['img_array'].values) / 255\n",
    "for i in range(0,test_arrays_std.shape[0]):\n",
    "    test_arrays_std[i] = (test_arrays_std[i] - Mean) / STD\n",
    "\n",
    "#Test-Valid data standardization\n",
    "test_valid_arrays_std = np.stack(blc_test_dataset_280x210['img_array'].values) / 255\n",
    "for i in range(0,test_valid_arrays_std.shape[0]):\n",
    "    test_valid_arrays_std[i] = (test_valid_arrays_std[i] - Mean) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean : [-0.01839491 -0.04231122 -0.04687724]   Test Set STD: [0.96394774 0.99849143 1.00558878]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = test_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = test_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Test Set Mean : {Mean_t}   Test Set STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_to_binary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jr/h5q7lybd23l1htckxf1x0dnr0000gn/T/ipykernel_45733/749138199.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_valid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblc_test_dataset_280x210\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_labels_binary\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_to_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mvalid_labels_binary\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_to_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtest_labels_binary\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_to_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_to_binary' is not defined"
     ]
    }
   ],
   "source": [
    "metda_keys = train_data[['type','image_id']].reset_index(drop=True), valid_data[['type','image_id']].reset_index(drop=True), test_data[['type','image_id']].reset_index(drop=True)\n",
    "\n",
    "train_arrays_std_T = torch.tensor(train_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "train_labels = torch.tensor(train_data['image_label'].values.astype(np.long))\n",
    "\n",
    "valid_arrays_std_T = torch.tensor(valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "valid_labels = torch.tensor(valid_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_valid_arrays_std_T = torch.tensor(test_valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_valid_labels = torch.tensor(blc_test_dataset_280x210['image_label'].values.astype(np.long))\n",
    "\n",
    "train_labels_binary =  torch.tensor(label_to_binary(train_labels))\n",
    "valid_labels_binary =  torch.tensor(label_to_binary(valid_labels))\n",
    "test_labels_binary =  torch.tensor(label_to_binary(test_labels))\n",
    "test_valid_labels_binary =  torch.tensor(label_to_binary(test_valid_labels))\n",
    "\n",
    "\n",
    "input_data = (train_arrays_std_T , valid_arrays_std_T, test_arrays_std_T ,metda_keys)\n",
    "labels = ((train_labels,valid_labels,test_labels), (train_labels_binary , valid_labels_binary ,test_labels_binary))\n",
    "\n",
    "balc_evaluation_set = (test_valid_arrays_std_T,test_valid_labels,test_valid_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = G_path + '/06_Rescaled_DataSet'\n",
    "\n",
    "try:\n",
    "    os.makedirs(dataset_training)\n",
    "except:\n",
    "    None\n",
    "\n",
    "pickle.dump((input_data,labels), open(dataset_training + '/Input_DataSet_280x210', 'wb'))\n",
    "pickle.dump(balc_evaluation_set, open(dataset_training + '/Balanced_Test_Set_280x210', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import AutoEncoders Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3833/3833 [00:05<00:00, 739.10it/s]\n",
      "100%|██████████| 407/407 [00:00<00:00, 748.03it/s]\n",
      "100%|██████████| 407/407 [00:00<00:00, 762.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def import_imges(img_path):\n",
    "    img_list = []\n",
    "    for path, dirs, files in  os.walk(img_path):\n",
    "        for f in files:\n",
    "            img_list.append( path +'/' + f)\n",
    "        for d in dirs:\n",
    "            img_list.append( path + d)\n",
    "    img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "    dataset = pd.DataFrame(columns=['image_id','img_array'])\n",
    "    for img in tqdm(img_list):\n",
    "        img_name = re.findall('\\w+', img)[-2:-1][0]\n",
    "        image = pl.Image.open( img)\n",
    "        img_array = np.asarray(image)\n",
    "        new_row = pd.Series({'image_id':img_name,'img_array':img_array}, name='')\n",
    "        dataset = dataset.append(new_row)\n",
    "    return dataset\n",
    "\n",
    "train_path = G_path + '/Augmented_AutoEncoded/train'\n",
    "valid_path = G_path + '/Augmented_AutoEncoded/valid'\n",
    "test_path = G_path + '/Augmented_AutoEncoded/test'\n",
    "\n",
    "train_set = import_imges(train_path)\n",
    "valid_set = import_imges(valid_path)\n",
    "test_set = import_imges(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = pd.concat([train_set,valid_set,test_set],axis=0)\n",
    "A_E_dataset = dataset_.merge(Meta_Data_Augmented, how='left', on='image_id')\n",
    "blc_test_dataset_A_E = dataset_[['image_id','img_array']].merge(blc_test_meta, how=\"right\",on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3833  Valid Size:  407  Test size:  407\n"
     ]
    }
   ],
   "source": [
    "df_ = A_E_dataset\n",
    "train_data = df_[df_.type=='train']\n",
    "valid_data = df_[df_.type=='valid']\n",
    "test_data = df_[df_.type=='test']\n",
    "print('Train size: ',train_data.shape[0] ,' Valid Size: ',valid_data.shape[0], ' Test size: ', test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.36115692 0.34408239 0.32405245]   STD: [0.16846211 0.26209011 0.24330585]\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.concat([train_data,valid_data], axis=0)\n",
    "training_arrays_scaled = np.stack(training_data['img_array'].values)  / 255\n",
    "Mean = training_arrays_scaled.mean(axis = (0,1,2)) \n",
    "STD = training_arrays_scaled.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean}   STD: {STD}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data standardization\n",
    "train_arrays_std = np.stack(train_data['img_array'].values) / 255\n",
    "for i in range(0,train_arrays_std.shape[0]):\n",
    "    train_arrays_std[i] = (train_arrays_std[i] - Mean) / STD\n",
    "#Validation data standardization\n",
    "valid_arrays_std = np.stack(valid_data['img_array'].values) / 255\n",
    "for i in range(0,valid_arrays_std.shape[0]):\n",
    "    valid_arrays_std[i] = (valid_arrays_std[i] - Mean) / STD\n",
    "#Test data standardization\n",
    "test_arrays_std = np.stack(test_data['img_array'].values) / 255\n",
    "for i in range(0,test_arrays_std.shape[0]):\n",
    "    test_arrays_std[i] = (test_arrays_std[i] - Mean) / STD\n",
    "\n",
    "#Test-Valid data standardization\n",
    "test_valid_arrays_std = np.stack(blc_test_dataset_A_E['img_array'].values) / 255\n",
    "for i in range(0,test_valid_arrays_std.shape[0]):\n",
    "    test_valid_arrays_std[i] = (test_valid_arrays_std[i] - Mean) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean : [-0.04389305 -0.05144053  0.03501551]   Test Set STD: [0.93557654 0.98728975 1.00074642]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = test_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = test_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Test Set Mean : {Mean_t}   Test Set STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metda_keys = train_data[['type','image_id']].reset_index(drop=True), valid_data[['type','image_id']].reset_index(drop=True), test_data[['type','image_id']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays_std_T = torch.tensor(train_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "train_labels = torch.tensor(train_data['image_label'].values.astype(np.long))\n",
    "\n",
    "valid_arrays_std_T = torch.tensor(valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "valid_labels = torch.tensor(valid_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_valid_arrays_std_T = torch.tensor(test_valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_valid_labels = torch.tensor(blc_test_dataset_A_E['image_label'].values.astype(np.long))\n",
    "\n",
    "A_E_input_data = (train_arrays_std_T , valid_arrays_std_T, test_arrays_std_T ,metda_keys)\n",
    "\n",
    "train_labels_binary =  torch.tensor(label_to_binary(train_labels))\n",
    "valid_labels_binary =  torch.tensor(label_to_binary(valid_labels))\n",
    "test_labels_binary =  torch.tensor(label_to_binary(test_labels))\n",
    "\n",
    "A_E_labels = ((train_labels,valid_labels,test_labels), (train_labels_binary , valid_labels_binary ,test_labels_binary))\n",
    "\n",
    "A_E_balc_test_set = (test_valid_arrays_std_T,test_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((A_E_input_data,A_E_labels), open(G_path + '/Augmented_AutoEncoded/'+ 'Latent_Input_Data_labels', 'wb'))\n",
    "pickle.dump(A_E_balc_test_set, open(G_path +  '/Augmented_AutoEncoded/'+  '/Evaluation_set_AutoEncoder', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
