{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSource_path = '/Users/sepehrbe/My_Drive/DataSources/SkinCare/archive'\n",
    "Directory_path = '/Users/sepehrbe/My_Drive/DataSources/SkinCare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from statistics import mean\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import pickle as pickle\n",
    "import PIL as pl\n",
    "import pandas as pd\n",
    "from PIL.Image import Transpose\n",
    "from PIL import Image\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Meta Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'HAM10000_metadata.csv'\n",
    "path, dir, files = os.walk(DataSource_path)\n",
    "meta_data_v0 = pd.read_csv(path[0] + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplication\n",
    "meta_data_noDup = meta_data_v0.groupby('lesion_id').first().reset_index()\n",
    "# Encode Labels\n",
    "meta_data_noDup.dx = pd.Categorical(meta_data_noDup.dx)\n",
    "meta_data_noDup['image_label'] = meta_data_noDup.dx.cat.codes\n",
    "# Reduce Nr. class 5 category to 2800\n",
    "class_5 = meta_data_noDup[meta_data_noDup.image_label==5]\n",
    "class_5_reduced = class_5.sample(n=2000, random_state=0)\n",
    "meta_data_noDup_reduced = pd.concat([meta_data_noDup[meta_data_noDup.image_label!=5],class_5_reduced],axis=0)\n",
    "# Get \n",
    "train_data, test_valid_data = train_test_split(meta_data_noDup_reduced, test_size = 0.2, random_state=0,stratify=meta_data_noDup_reduced['image_label'])\n",
    "test_data, valid_data = train_test_split(test_valid_data, test_size = 0.5, random_state=0,stratify=test_valid_data['image_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_noDup.to_csv(Directory_path+'Meta_data_noDup.csv')\n",
    "\n",
    "synthetics_ages = pd.read_csv(Directory_path+'/synthetics_ages.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['type'] = 'train'\n",
    "valid_data['type'] = 'valid'\n",
    "test_data['type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_valid_evaluation = pd.concat([valid_data,test_data],axis=0)\n",
    "test_valid_evaluation = test_valid_evaluation.groupby('image_label').head(15).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.491400\n",
       "2    0.176904\n",
       "4    0.152334\n",
       "1    0.078624\n",
       "0    0.056511\n",
       "6    0.024570\n",
       "3    0.019656\n",
       "Name: image_label, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.image_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "for path, dirs, files in  os.walk(DataSource_path):\n",
    "    for f in files:\n",
    "        img_list.append( path +'/' + f)\n",
    "    for d in dirs:\n",
    "        img_list.append( path + d)\n",
    "img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Images for Augmentation Directory\n",
    "Augment_Path = Directory_path + '/' + 'Augmentation'\n",
    "try:\n",
    "    os.makedirs(Augment_Path)\n",
    "except:\n",
    "    None\n",
    "try:\n",
    "    os.makedirs(Augment_Path+'/test')\n",
    "    os.makedirs(Augment_Path+'/valid')\n",
    "    os.makedirs(Augment_Path+'/train')\n",
    "except:\n",
    "    None\n",
    "labels = list(train_data.image_label.unique())\n",
    "\n",
    "for i in labels:\n",
    "    try:\n",
    "        os.makedirs(Augment_Path+'/test/'+str(i))\n",
    "        os.makedirs(Augment_Path+'/valid/'+str(i))\n",
    "        os.makedirs(Augment_Path+'/train/'+str(i))\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = train_data[['image_id','image_label']]\n",
    "name = 'train'\n",
    "def copy_to_AutoEncoder(data_ , name):\n",
    "    id_list = list(data_['image_id'])\n",
    "    id_labels = list(data_['image_label'])\n",
    "    for i in range(len(id_list)):\n",
    "        for j in img_list:\n",
    "            if id_list[i] in j:\n",
    "                shutil.copy(j, Augment_Path+'/' + name+ '/' + str(id_labels[i]) + '/' + id_list[i] + '.jpg')\n",
    "                break\n",
    "\n",
    "data_ = train_data[['image_id','image_label']]\n",
    "name = 'train'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)\n",
    "data_ = valid_data[['image_id','image_label']]\n",
    "name = 'valid'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)\n",
    "data_ = test_data[['image_id','image_label']]\n",
    "name = 'test'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = Augment_Path + '/train/'\n",
    "\n",
    "for i in [3, 6, 0, 1]:\n",
    "    f = folder_dir + str(i) + \"/\"\n",
    "    for image in os.listdir(f):\n",
    "        if \"ISIC\" in image and \"AUG_\" not in image and image.replace(\".jpg\", \"\"):\n",
    "            # print(\"Augmented image: \", image)\n",
    "            im1 = Image.open(f + image)\n",
    "            augmented = im1.transpose(Transpose.FLIP_LEFT_RIGHT)\n",
    "            augmented = augmented.transpose(Transpose.FLIP_TOP_BOTTOM)\n",
    "            augmented.save(folder_dir + \"/\" + str(i) + \"/AUG_\" + image)\n",
    "            # Add metadata of Augmented image\n",
    "            new_row = train_data[train_data.image_id == image.replace(\".jpg\", \"\")]\n",
    "            new_row['image_id'] = 'AUG_' + new_row['image_id'].iloc[0]\n",
    "            train_data = train_data.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_Data_Augmented = pd.concat([train_data, valid_data, test_data], axis=0)\n",
    "Meta_Data_Augmented.to_csv(Augment_Path+'/Meta_Data_Augmented.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Augmented Images and rescaling to 280x210**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4647/4647 [01:33<00:00, 49.44it/s]\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "for path, dirs, files in  os.walk(Augment_Path):\n",
    "    for f in files:\n",
    "        img_list.append( path +'/' + f)\n",
    "    for d in dirs:\n",
    "        img_list.append( path + d)\n",
    "img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "Images_280x210 = Directory_path + '/Images_280x210'\n",
    "\n",
    "try:\n",
    "    os.makedirs(Images_280x210)\n",
    "except:\n",
    "    None\n",
    "\n",
    "for img in tqdm(img_list):\n",
    "    file_out = Images_280x210 +'/'+ re.findall('\\w+',img)[-2:-1][0] +'.jpg'\n",
    "    os.system('convert %s %s %s' % ('-resize 46.66%',img,file_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Rescaled Images and convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [00:07<00:00, 603.39it/s]\n"
     ]
    }
   ],
   "source": [
    "img_list = os.listdir(Images_280x210)\n",
    "Augmented_280x210_dataset = pd.DataFrame(columns=['image_id','img_array'])\n",
    "for img in tqdm(img_list):\n",
    "    if '.jpg' in img:\n",
    "        img_name = img.replace('.jpg','')\n",
    "        image = pl.Image.open( Images_280x210 +'/'+ img)\n",
    "        img_array = np.asarray(image)\n",
    "        new_row = pd.Series({'image_id':img_name,'img_array':img_array}, name='')\n",
    "        Augmented_280x210_dataset = Augmented_280x210_dataset.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Augmented_280x210_dataset = Augmented_280x210_dataset.merge(Meta_Data_Augmented, how=\"left\",on='image_id')\n",
    "test_valid_dataset = Augmented_280x210_dataset.merge(test_valid_evaluation, how=\"right\",on='image_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Standardization / Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3833  Valid Size:  407  Test size:  407\n"
     ]
    }
   ],
   "source": [
    "df_ = Augmented_280x210_dataset\n",
    "train_data = df_[df_.type=='train']\n",
    "valid_data = df_[df_.type=='valid']\n",
    "test_data = df_[df_.type=='test']\n",
    "training_data = pd.concat([train_data,valid_data], axis=0)\n",
    "print('Train size: ',train_data.shape[0] ,' Valid Size: ',valid_data.shape[0], ' Test size: ', test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.76570269 0.55224556 0.57657015]   STD: [0.13616887 0.1481389  0.16343116]\n"
     ]
    }
   ],
   "source": [
    "training_arrays_scaled = np.stack(training_data['img_array'].values)  / 255\n",
    "Mean = training_arrays_scaled.mean(axis = (0,1,2)) \n",
    "STD = training_arrays_scaled.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean}   STD: {STD}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data standardization\n",
    "train_arrays_std = np.stack(train_data['img_array'].values) / 255\n",
    "for i in range(0,train_arrays_std.shape[0]):\n",
    "    train_arrays_std[i] = (train_arrays_std[i] - Mean) / STD\n",
    "#Validation data standardization\n",
    "valid_arrays_std = np.stack(valid_data['img_array'].values) / 255\n",
    "for i in range(0,valid_arrays_std.shape[0]):\n",
    "    valid_arrays_std[i] = (valid_arrays_std[i] - Mean) / STD\n",
    "#Test data standardization\n",
    "test_arrays_std = np.stack(test_data['img_array'].values) / 255\n",
    "for i in range(0,test_arrays_std.shape[0]):\n",
    "    test_arrays_std[i] = (test_arrays_std[i] - Mean) / STD\n",
    "\n",
    "#Test-Valid data standardization\n",
    "test_valid_arrays_std = np.stack(test_valid_dataset['img_array'].values) / 255\n",
    "for i in range(0,test_valid_arrays_std.shape[0]):\n",
    "    test_valid_arrays_std[i] = (test_valid_arrays_std[i] - Mean) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.00354374 0.00936217 0.00933908]   STD: [0.99341258 0.99663028 0.99648389]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = train_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = train_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean_t}   STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [-0.01839491 -0.04231122 -0.04687724]   STD: [0.96394774 0.99849143 1.00558878]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = test_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = test_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean_t}   STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays_std_T = torch.tensor(train_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "train_labels = torch.tensor(train_data['image_label'].values.astype(np.long))\n",
    "\n",
    "valid_arrays_std_T = torch.tensor(valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "valid_labels = torch.tensor(valid_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_valid_arrays_std_T = torch.tensor(test_valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_valid_labels = torch.tensor(test_valid_evaluation['image_label'].values.astype(np.long))\n",
    "\n",
    "input_data = (train_arrays_std_T , valid_arrays_std_T, test_arrays_std_T )\n",
    "labels = (train_labels,valid_labels,test_labels)\n",
    "\n",
    "evaluation_set = (test_valid_arrays_std_T,test_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = Directory_path + '/DataSet_280x210'\n",
    "\n",
    "try:\n",
    "    os.makedirs(dataset_training)\n",
    "except:\n",
    "    None\n",
    "\n",
    "pickle.dump(input_data, open(dataset_training + '/InputData_280x210', 'wb'))\n",
    "pickle.dump(labels, open(dataset_training + '/Labels_280x210', 'wb'))\n",
    "pickle.dump(evaluation_set, open(dataset_training + '/Evaluation_set', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 3, 280, 210])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58800"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "280*210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58374"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "207*282"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
