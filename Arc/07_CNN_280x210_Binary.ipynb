{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzRoZIkXDiT7",
        "outputId": "bae35d96-cb3e-4d62-d817-1fc69bb17f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[K     |████████████████████████████████| 256 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  G_path = '/content/drive/MyDrive/DataSources/Project_Data'\n",
        "  !pip install pickle5\n",
        "except:\n",
        "  G_path = '/content/drive/MyDrive/DataSources/Project_Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VZrZXitoAlDd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset as dset\n",
        "#import torchvision.transforms.Compose\n",
        "import numpy as np\n",
        "from datetime import datetime\t\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pickle as pickle\n",
        "from statistics import mean\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 250\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6y2NEQ6WOvNi"
      },
      "outputs": [],
      "source": [
        "Binary_classification = True\n",
        "file = 'Input_DataSet_280x210' \n",
        "input_data , labels = pickle.load(open(G_path + '/06_Rescaled_DataSet/'+ file,'rb'))\n",
        "file = 'Balanced_Test_Set_280x210' \n",
        "Evaluation_set = pickle.load(open(G_path + '/06_Rescaled_DataSet/' + file,'rb'))\n",
        "\n",
        "if Binary_classification:\n",
        "  labels = labels[1]\n",
        "else:\n",
        "  labels = labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Mz7iN2hFgF",
        "outputId": "49a4b417-4827-4604-8341-20e83a9da4ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels[1][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX40UaIYFt0J"
      },
      "source": [
        "# **CNN Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0B7S_srvzD6I"
      },
      "outputs": [],
      "source": [
        "class CNN_Nets:\n",
        "  class Net8(nn.Module):\n",
        "    def __init__(self,drop_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d( in_channels=3 , out_channels=64, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool1 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv2 = nn.Conv2d( in_channels=64, out_channels=128, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv3 = nn.Conv2d( in_channels=128, out_channels=64, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool3 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.fc1   = nn.Linear(in_features= 384 , out_features = 128)\n",
        "        self.fc2   = nn.Linear(in_features= 128 , out_features = 7)\n",
        "        self.Act   = nn.LeakyReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "    def forward(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.Act(self.pool1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.Act(self.pool2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.Act(self.pool3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "    def forward_noDrop(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.Act(self.pool1(out))\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.Act(self.pool2(out))\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.Act(self.pool3(out))\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "  class Net8_a(nn.Module):\n",
        "    def __init__(self,drop_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d( in_channels=3 , out_channels=64, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool1 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv2 = nn.Conv2d( in_channels=64, out_channels=128, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv3 = nn.Conv2d( in_channels=128, out_channels=256, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool3 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.fc1   = nn.Linear(in_features= 1536 , out_features = 128)\n",
        "        self.fc2   = nn.Linear(in_features= 128 , out_features = 7)\n",
        "        self.Act   = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "    def forward(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.pool1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.pool2(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.pool3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "    def forward_noDrop(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.pool1(out)\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.pool2(out)\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.pool3(out)\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "  \n",
        "  class Net8_b(nn.Module):\n",
        "    def __init__(self,drop_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d( in_channels=3 , out_channels=64, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool1 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv2 = nn.Conv2d( in_channels=64, out_channels=128, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv3 = nn.Conv2d( in_channels=128, out_channels=256, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool3 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.fc1   = nn.Linear(in_features= 1536 , out_features = 128)\n",
        "        self.fc2   = nn.Linear(in_features= 128 , out_features = 7)\n",
        "        self.Act   = nn.LeakyReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "    def forward(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.Act(self.pool1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.Act(self.pool2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.Act(self.pool3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "    def forward_noDrop(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.Act(self.pool1(out))\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.Act(self.pool2(out))\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.Act(self.pool3(out))\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "  class Net8_b_binary(nn.Module):\n",
        "    def __init__(self,drop_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d( in_channels=3 , out_channels=64, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool1 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv2 = nn.Conv2d( in_channels=64, out_channels=128, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool2 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.conv3 = nn.Conv2d( in_channels=128, out_channels=256, stride = 2 , kernel_size=(3, 3))\n",
        "        self.pool3 = nn.MaxPool2d( kernel_size = (3,3), stride = 2, padding = 0 )\n",
        "        self.fc1   = nn.Linear(in_features= 1536 , out_features = 128)\n",
        "        self.fc2   = nn.Linear(in_features= 128 , out_features = 2)\n",
        "        self.Act   = nn.LeakyReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "    def forward(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.Act(self.pool1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.Act(self.pool2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.Act(self.pool3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "    def forward_noDrop(self, x):\n",
        "        out = self.Act(self.conv1(x))\n",
        "        out = self.Act(self.pool1(out))\n",
        "        out = self.Act(self.conv2(out))\n",
        "        out = self.Act(self.pool2(out))\n",
        "        out = self.Act(self.conv3(out))\n",
        "        out = self.Act(self.pool3(out))\n",
        "        out = torch.flatten(out, 1) \n",
        "        out = self.Act(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Zfb1SkAlDg"
      },
      "source": [
        "# **Test Nets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJTfTtmfAlDg",
        "outputId": "701e6898-65f2-4de5-f7ea-31610778a096"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 7])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_test = CNN_Nets.Net8_a(drop_out=0.1)\n",
        "X_ = input_data[0][0:5]\n",
        "out = model_test(X_)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWC5a989AlDg"
      },
      "source": [
        "# **Metrics and Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pBE96mxDAlDg"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(Y,Y_pred):\n",
        "    CM = sk.metrics.confusion_matrix(Y,Y_pred)\n",
        "    print('Nr. of Data : \\n', CM.sum())\n",
        "    print('Accuracy of The Model : \\n', np.diag(CM).sum()/CM.sum())\n",
        "    sk.metrics.ConfusionMatrixDisplay(CM).plot()\n",
        "    FP = CM.sum(axis=0) - np.diag(CM) \n",
        "    FN = CM.sum(axis=1) - np.diag(CM)\n",
        "    TP = np.diag(CM)\n",
        "    TN = CM.sum() - (FP + FN + TP)\n",
        "    weights = CM.sum(axis=1) / CM.sum() \n",
        "    ACC = np.nan_to_num((TP+TN)/(TP+FP+FN+TN) , nan=0)\n",
        "    Recall_Sensitivity = np.nan_to_num(TP/(TP+FN) , nan=0)\n",
        "    Specificity = np.nan_to_num(TN/(TN+FP) , nan=0)\n",
        "    Precision = np.nan_to_num(TP/(TP+FP) , nan=0)\n",
        "    f1_score = np.nan_to_num( 2*Precision*Recall_Sensitivity / (Recall_Sensitivity + Precision), nan=0)\n",
        "    Performance_DF = pd.concat([pd.DataFrame(CM),pd.DataFrame(weights, columns=['weights']),pd.DataFrame(Precision, columns=['Precision']),pd.DataFrame(Recall_Sensitivity,columns=['Recall_Sensitivity'])\n",
        "        ,pd.DataFrame(Specificity, columns=['Specificity']),pd.DataFrame(f1_score, columns=['f1_score'])], axis=1)\n",
        "    total_row1 = pd.Series({'Precision':mean(Precision),'Recall_Sensitivity':mean(Recall_Sensitivity),'Specificity':mean(Specificity),'f1_score':mean(f1_score)}, name='Simple Avg.')\n",
        "    total_row2 = pd.Series({'Precision':sum(weights*Precision),'Recall_Sensitivity':sum(weights*Recall_Sensitivity),'Specificity':sum(weights*Specificity),'f1_score':sum(weights*f1_score)}, name='Weighted Avg.')\n",
        "    Performance_DF = Performance_DF.append([total_row1,total_row2])\n",
        "    cols = ['weights','Precision','Recall_Sensitivity','Specificity','f1_score']\n",
        "    per_details = Performance_DF[cols].style.format({'weights': \"{:.1%}\",'Precision': \"{:.1%}\",'Recall_Sensitivity': \"{:.1%}\",'Specificity': \"{:.1%}\",'f1_score': \"{:.1%}\"})\n",
        "    return per_details\n",
        "\n",
        "def recall_specificity(Y,Y_pred, type):\n",
        "    CM = sk.metrics.confusion_matrix(Y,Y_pred)\n",
        "    FP = CM.sum(axis=0) - np.diag(CM) \n",
        "    FN = CM.sum(axis=1) - np.diag(CM)\n",
        "    TP = np.diag(CM)\n",
        "    TN = CM.sum() - (FP + FN + TP)\n",
        "    weights = CM.sum(axis=1) / CM.sum() \n",
        "    ACC = np.nan_to_num((TP+TN)/(TP+FP+FN+TN) , nan=0)\n",
        "    Recall_Sensitivity = np.nan_to_num(TP/(TP+FN) , nan=0)\n",
        "    Specificity = np.nan_to_num(TN/(TN+FP) , nan=0)\n",
        "    Precision = np.nan_to_num(TP/(TP+FP) , nan=0)\n",
        "    f1_score = np.nan_to_num( 2*Precision*Recall_Sensitivity / (Recall_Sensitivity + Precision), nan=0)\n",
        "    Performance_DF = pd.concat([pd.DataFrame(CM),pd.DataFrame(weights, columns=['weights']),pd.DataFrame(Precision, columns=['Precision']),pd.DataFrame(Recall_Sensitivity,columns=['Recall_Sensitivity'])\n",
        "        ,pd.DataFrame(Specificity, columns=['Specificity']),pd.DataFrame(f1_score, columns=['f1_score'])], axis=1)\n",
        "    total_row1 = pd.Series({'Precision':mean(Precision),'Recall_Sensitivity':mean(Recall_Sensitivity),'Specificity':mean(Specificity),'f1_score':mean(f1_score)}, name='Simple Avg.')\n",
        "    total_row2 = pd.Series({'Precision':sum(weights*Precision),'Recall_Sensitivity':sum(weights*Recall_Sensitivity),'Specificity':sum(weights*Specificity),'f1_score':sum(weights*f1_score)}, name='Weighted Avg.')\n",
        "    Performance_DF = Performance_DF.append([total_row1,total_row2])\n",
        "    cols = ['weights','Precision','Recall_Sensitivity','Specificity','f1_score']\n",
        "    return per_details\n",
        "\n",
        "def plot_loss_accuracy(model_):\n",
        "    epochs_X = [i for i in range(1, len(model_.Epochs_Train_loss)+1)]\n",
        "    fig, axs = plt.subplots(1,2,figsize=(14,4))\n",
        "    axs[0].plot(epochs_X , model_.Epochs_Train_loss , 'bo-', label='Train loss')\n",
        "    axs[0].plot(epochs_X , model_.Epochs_Val_loss,'ro-', label='Validation loss')\n",
        "    axs[0].plot(epochs_X , model_.Epochs_test_loss,'go-', label='Test loss')\n",
        "    axs[0].set_xlabel(\"Epochs\", fontsize = 12)\n",
        "    axs[0].set_ylabel(\"Loss\", fontsize = 12)\n",
        "    axs[0].grid()\n",
        "    axs[0].legend()\n",
        "    axs[0].set_title('Train and Validation loss by epochs', fontsize = 14)\n",
        "    axs[1].plot(epochs_X , model_.Epochs_Train_Acc , 'bo-', label='Train Accuracy')\n",
        "    axs[1].plot(epochs_X , model_.Epochs_Val_Acc ,'ro-', label='Validation Accuracy')\n",
        "    axs[1].plot(epochs_X , model_.Epochs_test_Acc ,'go-', label='Test Accuracy')\n",
        "    axs[1].set_xlabel(\"Epochs\", fontsize = 12)\n",
        "    axs[1].set_ylabel(\"Accuracy\", fontsize = 12)\n",
        "    axs[1].grid()\n",
        "    axs[1].legend()\n",
        "    axs[1].set_title('Train and Validation Accuracy by epochs', fontsize = 14)\n",
        "    plt.show()\n",
        "\n",
        "def recall_specificity_precision(Y,Y_pred, weighted_avg):\n",
        "    CM = sk.metrics.confusion_matrix(Y,Y_pred)\n",
        "    FP = CM.sum(axis=0) - np.diag(CM) \n",
        "    FN = CM.sum(axis=1) - np.diag(CM)\n",
        "    TP = np.diag(CM)\n",
        "    TN = CM.sum() - (FP + FN + TP)\n",
        "    weights = CM.sum(axis=1) / CM.sum() \n",
        "    Accuracy = np.nan_to_num((TP+TN)/(TP+FP+FN+TN) , nan=0)\n",
        "    Recall = np.nan_to_num(TP/(TP+FN) , nan=0)\n",
        "    Specificity = np.nan_to_num(TN/(TN+FP) , nan=0)\n",
        "    Precision = np.nan_to_num(TP/(TP+FP) , nan=0)\n",
        "    if weighted_avg:\n",
        "        return round(sum(weights*Recall),3), round(sum(weights*Specificity),3), round(sum(weights*Precision),3)\n",
        "    else:\n",
        "        return round(mean(Recall),3), round(mean(Specificity),3), round(mean(Precision),3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XV23bjxXUi7"
      },
      "source": [
        "### **1.3 Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I0bPoj1nz3Or"
      },
      "outputs": [],
      "source": [
        "class Model_Training_with_loader:\n",
        "\n",
        "    def __init__(self, Net, Drop, LR, batch_size , Momentum, epochs,patience, weight_decay, loss_func, opt_func,w_sampler, trainDataset, validDataset, X_test,Y_test, print_epochs,hyper_params):    \n",
        "        \n",
        "        self.model = Net(Drop).to(device)\n",
        "        if opt_func is torch.optim.Adam:\n",
        "            self.opt = opt_func(self.model.parameters(), lr=LR, weight_decay=weight_decay)\n",
        "        else:\n",
        "            self.opt = opt_func(self.model.parameters(), lr=LR,momentum=Momentum, weight_decay=weight_decay)\n",
        "\n",
        "        self.loss_func = loss_func()\n",
        "        self.epochs = epochs\n",
        "        self.patience = patience\n",
        "        self.print_epochs = print_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.Epochs_Train_loss = []\n",
        "        self.Epochs_Train_Acc = []\n",
        "        self.Epochs_Val_loss = []\n",
        "        self.Epochs_Val_Acc = []\n",
        "        self.Epochs_test_loss = []\n",
        "        self.Epochs_test_Acc = []\n",
        "        self.hyper_params = hyper_params\n",
        "        self.Y_test = Y_test\n",
        "        self.X_test = X_test\n",
        "        self.train_loader = DataLoader(dataset = trainDataset , sampler = w_sampler, batch_size = self.batch_size, num_workers=4)\n",
        "        self.valid_loader = DataLoader(dataset = validDataset , shuffle=True, batch_size = self.batch_size, num_workers=2)\n",
        "        self.test_loader = DataLoader(dataset = testDataset , shuffle=True, batch_size = self.batch_size, num_workers=2)\n",
        "\n",
        "    def train(self):\n",
        "        \n",
        "        model = self.model\n",
        "        loss_fn = self.loss_func\n",
        "        opt = self.opt \n",
        "        batch_size = self.batch_size\n",
        "        min_loss = 100\n",
        "        iters = 0\n",
        "\n",
        "        for epoch in range(1, self.epochs+1 ):\n",
        "            start_time=time.time()\n",
        "            steps_train_loss = []\n",
        "            steps_train_Acc = []\n",
        "            steps_val_loss = []\n",
        "            steps_val_Acc = []\n",
        "            steps_test_loss = []\n",
        "            steps_test_Acc = []\n",
        "            torch.cuda.empty_cache()\n",
        "            for batch, (X, Y) in enumerate(self.train_loader):\n",
        "                X = X.to(device)\n",
        "                Y = Y.to(device)\n",
        "                opt.zero_grad()\n",
        "                model.train()\n",
        "                y_pred = model.forward(X)\n",
        "                loss = loss_fn(y_pred, Y)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                y_pred = y_pred.argmax(axis=1)\n",
        "                nr_of_corrects = (y_pred == Y).sum().item()\n",
        "                step_acc = nr_of_corrects / batch_size\n",
        "                steps_train_Acc.append(step_acc)\n",
        "                steps_train_loss.append(loss.item())\n",
        "                \n",
        "                # if (i+1) % 200 == 0:    # print every 2000 mini-batches\n",
        "                #     print('[Epoch: {}, Nr. Batch: {}]  , Train-Steps-loss: {:.1f} , running_acc: {:.1%}'.format(epoch , i+1 , train_steps_loss , batch_nr_correct / train_nr_total))\n",
        "                #     self.train_steps_acc = []\n",
        "                #     train_steps_loss = 0\n",
        "\n",
        "              #validation loss calculation\n",
        "            \n",
        "            for batch, (X, Y) in enumerate(self.valid_loader):\n",
        "                X = X.to(device)\n",
        "                Y = Y.to(device)\n",
        "                model.eval()\n",
        "                Y_pred = model(X)\n",
        "                loss_ = loss_fn(Y_pred, Y)\n",
        "                epoch_loss = loss_.item()\n",
        "                Y_pred = Y_pred.argmax(axis=1)\n",
        "                nr_correct = (Y_pred == Y).sum().item()\n",
        "                step_acc = nr_correct / batch_size\n",
        "                steps_val_Acc.append(step_acc)\n",
        "                steps_val_loss.append(epoch_loss)\n",
        "                \n",
        "            #Test Set Performance\n",
        "            for batch, (X, Y) in enumerate(self.test_loader):\n",
        "                X = X.to(device)\n",
        "                Y = Y.to(device)\n",
        "                model.eval()\n",
        "                Y_pred = model(X)\n",
        "                loss_ = loss_fn(Y_pred, Y)\n",
        "                epoch_loss = loss_.item()\n",
        "                Y_pred = Y_pred.argmax(axis=1)\n",
        "                nr_correct = (Y_pred == Y).sum().item()\n",
        "                step_acc = nr_correct / batch_size\n",
        "                steps_test_Acc.append(step_acc)\n",
        "                steps_test_loss.append(epoch_loss)\n",
        "\n",
        "            # Epoch Performance Metrics\n",
        "            train_epoch_loss = mean(steps_train_loss)\n",
        "            train_epoch_Acc = mean(steps_train_Acc)\n",
        "            self.Epochs_Train_loss.append(train_epoch_loss)\n",
        "            self.Epochs_Train_Acc.append(train_epoch_Acc)   \n",
        "            val_epoch_loss = mean(steps_val_loss)\n",
        "            val_epoch_Acc = mean(steps_val_Acc)\n",
        "            self.Epochs_Val_loss.append(val_epoch_loss)\n",
        "            self.Epochs_Val_Acc.append(val_epoch_Acc)\n",
        "            test_epoch_loss = mean(steps_test_loss)\n",
        "            test_epoch_Acc = mean(steps_test_Acc)           \n",
        "            self.Epochs_test_loss.append(test_epoch_loss)\n",
        "            self.Epochs_test_Acc.append(test_epoch_Acc)\n",
        "            End_time = time.time() \n",
        "\n",
        "            if val_epoch_loss < min_loss:\n",
        "              min_loss = val_epoch_loss\n",
        "              pickle.dump(model,open('Best_Model','wb'))\n",
        "              iters = 0\n",
        "            else:\n",
        "              iters +=1\n",
        "     \n",
        "            if iters > self.patience:\n",
        "              model = pickle.load(open('Best_Model','rb'))\n",
        "              print(f'Earlt Stoppo happen at Epoche {epoch} after no improvment of {iters} epochs ')\n",
        "              break\n",
        "\n",
        "            if self.print_epochs:\n",
        "                print(f'[Epoch: {epoch}]  , Train_loss: {train_epoch_loss:.1f} , Train_Acc: {train_epoch_Acc:.1%}, Val_loss: {val_epoch_loss:.1f} , Val_Acc: {val_epoch_Acc:.1%}, Test_Acc: {test_epoch_Acc:.1%}  , run time: {np.round(End_time - start_time, 2)}')\n",
        "        # print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-7sG3Slop1"
      },
      "source": [
        "# **Define Training data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sM2zFYZBWR45"
      },
      "outputs": [],
      "source": [
        "X_train = input_data[0]\n",
        "Y_train = labels[0]\n",
        "X_valid = input_data[1]\n",
        "Y_valid = labels[1]\n",
        "X_test = input_data[2]\n",
        "Y_test = labels[2]\n",
        "\n",
        "\n",
        "label_freq = np.bincount(Y_train)\n",
        "labels_weights = 1. / label_freq\n",
        "weights = labels_weights[Y_train]\n",
        "w_sampler = WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "trainDataset = dset(X_train, Y_train)\n",
        "validDataset = dset(X_valid, Y_valid)\n",
        "testDataset = dset(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geKyTRJ8AlDh"
      },
      "source": [
        "# **Model by Grid**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VL5FxPLAAlDi"
      },
      "outputs": [],
      "source": [
        "\n",
        "Net = [CNN_Nets.Net8_b_binary]\n",
        "Drop = [0.2,0.25]\n",
        "LR = [ 1.1e-3,1e-2,1e-4]\n",
        "batch_size = [24,32]\n",
        "Momentum = [0.77,.85]\n",
        "epochs = [100]\n",
        "patience = [20]\n",
        "weight_decay = [1e-3,1e-5]\n",
        "loss_func  =  [nn.CrossEntropyLoss]\n",
        "opt_func = [torch.optim.SGD]\n",
        "\n",
        "\n",
        "grid = {\n",
        "    'Net' : Net\n",
        "    ,'Drop' : Drop\n",
        "    ,'LR' : LR\n",
        "    ,'batch_size' : batch_size\n",
        "    ,'Momentum' : Momentum\n",
        "    ,'epochs' : epochs\n",
        "    ,'patience': patience\n",
        "    ,'weight_decay' :weight_decay\n",
        "    ,'loss_func'  :loss_func\n",
        "    ,'opt_func' : opt_func\n",
        "}\n",
        "params = sk.model_selection.ParameterGrid(grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEkHc_7xAlDi",
        "outputId": "9264b58a-b3cf-4971-e47a-6397aeccf9d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Max_Test_Recall: 27.6%, Max_BLC_Test_Recall: 29.5%, Max_Tr_Acc: 21.5%, Max_V_Acc: 52.1%, Max_Te_Acc: 49.4% : 100%|██████████| 4/4 [01:10<00:00, 17.51s/it, Train_Acc: 20.0% , Valide_Acc: 46.9% , Test_Acc: 42.5%]\n"
          ]
        }
      ],
      "source": [
        "print_epochs = False\n",
        "export_csv = True\n",
        "save_models = True\n",
        "export_name = 'CNN_280x210_Binary' # {'280_210_','AutoEncoder_'}\n",
        "\n",
        "Hyper_Details = pd.DataFrame(columns=['model_name','hyper_param','test_overall_metric','test_recall','blc_test_recall','valid_recall','train_accuracy','valid_accuracy','test_accuracy','train_epoch_loss','train_epoch_acc','valid_epoch_loss','valid_epoch_acc','test_epoch_acc','test_epoch_loss'])\n",
        "date_hour = datetime.now().strftime(\"%d_%b%y_%H-%M\") \n",
        "Max_train_Acc = 0  \n",
        "Max_valid_Acc = 0 \n",
        "Max_test_Acc = 0                  \n",
        "Max_test_Recall = 0\n",
        "Max_BLC_Test_Recall = 0\n",
        "Max_test_overall_metric = 0\n",
        "i = -1\n",
        "pbar = tqdm(params)\n",
        "for p in pbar:\n",
        "    i += 1\n",
        "    model_name =  export_name + 'Model' + str(i) + '_' + date_hour\n",
        "    torch.cuda.empty_cache()\n",
        "    Model_ = Model_Training_with_loader(**p,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=p)\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    Model_.train()\n",
        "\n",
        "    train_accuracy = max(Model_.Epochs_Train_Acc)\n",
        "    # valid_accuracy = max(Model_.Epochs_Val_Acc)\n",
        "    # test_accuracy = max(Model_.Epochs_test_Acc)\n",
        "    train_epoch_loss = Model_.Epochs_Train_loss\n",
        "    train_epoch_acc = Model_.Epochs_Train_Acc\n",
        "    valid_epoch_loss = Model_.Epochs_Val_loss\n",
        "    valid_epoch_acc = Model_.Epochs_Val_Acc\n",
        "    test_epoch_loss = Model_.Epochs_test_loss\n",
        "    test_epoch_acc = Model_.Epochs_test_Acc\n",
        "\n",
        "\n",
        "    model_ = Model_.model.eval().to('cpu')\n",
        "    # test Avg. Sensitivity\n",
        "    X = input_data[2]\n",
        "    Y = labels[2]\n",
        "    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n",
        "    test_accuracy = sk.metrics.accuracy_score(Y, Y_pred )\n",
        "    test_precision, test_recall, test_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n",
        "    # Valid Avg. Sensitivity\n",
        "    X = input_data[1]\n",
        "    Y = labels[1]\n",
        "    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n",
        "    valid_accuracy = sk.metrics.accuracy_score(Y, Y_pred )\n",
        "    valid_precision, valid_recall, valid_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n",
        "    # Balanced test set Avg. Sensitivity\n",
        "    X = Evaluation_set[0]\n",
        "    Y = Evaluation_set[1]\n",
        "    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n",
        "    blc_test_precision, blc_test_recall, blc_test_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n",
        "\n",
        "    test_overall_metric = (test_accuracy + test_recall + blc_test_recall ) / 3\n",
        "    if test_overall_metric > Max_test_overall_metric: pickle.dump(model_ , open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  'Best_Grid_Model' +str(i) +'_' + date_hour, 'wb'))\n",
        "\n",
        "    if train_accuracy > Max_train_Acc: Max_train_Acc = train_accuracy\n",
        "    if valid_accuracy > Max_valid_Acc: Max_valid_Acc = valid_accuracy\n",
        "    if test_accuracy > Max_test_Acc: Max_test_Acc = test_accuracy\n",
        "    if test_recall > Max_test_Recall: Max_test_Recall = test_recall\n",
        "    if blc_test_recall > Max_BLC_Test_Recall: Max_BLC_Test_Recall = blc_test_recall\n",
        "\n",
        "    pbar.set_description(f'Max_Test_Recall: {Max_test_Recall:.1%}, Max_BLC_Test_Recall: {Max_BLC_Test_Recall:.1%}, Max_Tr_Acc: {Max_train_Acc:.1%}, Max_V_Acc: {Max_valid_Acc:.1%}, Max_Te_Acc: {Max_test_Acc:.1%} ')\n",
        "    pbar.set_postfix_str(f'Train_Acc: {train_accuracy:.1%} , Valide_Acc: {valid_accuracy:.1%} , Test_Acc: {test_accuracy:.1%}')\n",
        "\n",
        "    new_row = pd.Series({'model_name':model_name,'hyper_param':p,'test_overall_metric':test_overall_metric,'test_recall':test_recall,'blc_test_recall':blc_test_recall,'valid_recall':valid_recall,'train_accuracy':train_accuracy,'valid_accuracy':valid_accuracy,'test_accuracy':test_accuracy,\n",
        "                         'train_epoch_loss':train_epoch_loss,'train_epoch_acc':train_epoch_acc,'valid_epoch_loss':valid_epoch_loss,'valid_epoch_acc':valid_epoch_acc,'test_epoch_acc':test_epoch_acc,'test_epoch_loss':test_epoch_loss}, name='')\n",
        "    Hyper_Details = Hyper_Details.append(new_row)\n",
        "    m_details = pd.DataFrame()\n",
        "    m_details = m_details.append(new_row)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if export_csv:\n",
        "      # Hyper_Details.to_csv(G_path +'/Saved/Grid_Search_Results/' + export_name + date_hour+ '.csv')\n",
        "      \n",
        "      pickle.dump(m_details, open(G_path +'/08_Saved_Models_Outpus/Grid_Search_Results/' + model_name, 'wb'))\n",
        "      pickle.dump(Hyper_Details, open(G_path +'/08_Saved_Models_Outpus/Grid_Search_Results/' + 'Grid_All_' + export_name + date_hour , 'wb'))\n",
        "\n",
        "    if save_models:\n",
        "      pickle.dump(model_, open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  model_name, 'wb'))\n",
        "\n",
        "    Best_Grid_Model = pickle.load(open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  'Best_Grid_Model' +str(i) +'_' + date_hour, 'rb'))\n",
        "\n",
        "Hyper_Details.sort_values('test_overall_metric', ascending=False,inplace=True)\n",
        "best_param = Hyper_Details['hyper_param'][0]\n",
        "best_params = Hyper_Details['hyper_param'][:4]\n",
        "\n",
        "# Grid_Details = Hyper_Details\n",
        "\n",
        "# Model_Grid = Model_Training_with_loader(**best_param,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=best_param)\n",
        "# np.random.seed(0)\n",
        "# random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# Model_Grid.train()\n",
        "# print('Max Avg. Recall on Test ====>> :')\n",
        "# Grid_Details['test_recall'][:5], Grid_Details['hyper_param'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVnEzpk9FGTb"
      },
      "outputs": [],
      "source": [
        "# model_ = Model_Grid\n",
        "model_1 = Best_Grid_Model\n",
        "X = input_data[2]\n",
        "Y = labels[2]\n",
        "# X = Evaluation_set[0]\n",
        "# Y = Evaluation_set[1]\n",
        "Y_pred = model_1.forward_noDrop(X).argmax(dim=1)\n",
        "results = confusion_matrix(Y,Y_pred)\n",
        "# plot_loss_accuracy(model_)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zomymNkU4JNX"
      },
      "source": [
        "# **Cross Validation: Repeat 5 times with 10 Kfold (overall 50 models)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLfqTs-54anr"
      },
      "outputs": [],
      "source": [
        "# K_Folds_test = sk.model_selection.KFold(n_splits=10, shuffle=True)\n",
        "K_Folds_valid = sk.model_selection.KFold(n_splits=10, shuffle=True)\n",
        "dat_hour = datetime.now().strftime(\"%d_%b_%Y_%H_%M\")\n",
        "print_epochs = False\n",
        "\n",
        "param1 = {\n",
        "    'Net' : CNN_Nets.Net8_b_binary\n",
        "    ,'Drop' : 0.2\n",
        "    ,'LR' : 1.3e-3\n",
        "    ,'batch_size' : 20\n",
        "    ,'Momentum' : 0.85\n",
        "    ,'epochs' : 100\n",
        "    ,'patience': 20\n",
        "    ,'weight_decay' :1e-3\n",
        "    ,'loss_func'  :torch.nn.modules.loss.CrossEntropyLoss\n",
        "    ,'opt_func' : torch.optim.SGD\n",
        "}\n",
        "\n",
        "CV_data = torch.cat([input_data[0],input_data[1]],dim=0)\n",
        "CV_label = torch.cat([labels[0],labels[1]],dim=0)\n",
        "X_test = input_data[2]\n",
        "Y_test = labels[2]\n",
        "\n",
        "\n",
        "CV_Details = pd.DataFrame(columns=['hyper_param','train_recall_weighed','valid_recall_weighed','test_recall_weighed'\n",
        "                                  ,'valid_recall_simple','test_recall_simple'\n",
        "                                  ,'valid_specificity_weighed','test_specificity_weighed'\n",
        "                                  ,'valid_specificity_simple','test_specificity_simple'])\n",
        "\n",
        "Max_train_Acc = 0  \n",
        "Max_valid_Acc = 0 \n",
        "Max_test_Acc = 0                  \n",
        "\n",
        "pbar1 = tqdm(range(5))\n",
        "for i in pbar1:\n",
        "    pbar2 = tqdm(enumerate(K_Folds_valid.split(CV_data)), total=K_Folds_valid.get_n_splits())\n",
        "    for fold, (training_index, valid_index) in pbar2:\n",
        "        X_train = CV_data[training_index]\n",
        "        Y_train = CV_label[training_index]\n",
        "        X_valid = CV_data[valid_index]\n",
        "        Y_valid = CV_label[valid_index]\n",
        "\n",
        "        label_freq = np.bincount(Y_train)\n",
        "        labels_weights = 1. / label_freq\n",
        "        weights = labels_weights[Y_train]\n",
        "        w_sampler = WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "        trainDataset = dset(X_train, Y_train)\n",
        "        validDataset = dset(X_valid, Y_valid)\n",
        "        testDataset = dset(X_test, Y_test)\n",
        "\n",
        "        Model_ = Model_Training_with_loader(**param1,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=param1)\n",
        "        Model_.train()\n",
        "\n",
        "        model_ = Model_.model.eval().to('cpu')\n",
        "            \n",
        "        # Train\n",
        "        train_recall_weighed = mean(Model_.Epochs_Train_Acc)\n",
        "\n",
        "        # Valid\n",
        "        Y_pred = model_(X_valid).argmax(axis=1)\n",
        "        Y = Y_valid\n",
        "        valid_recall_weighed, valid_specificity_weighed, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=True)\n",
        "        valid_recall_simple, valid_specificity_simple, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=False)\n",
        "\n",
        "        # Test\n",
        "        Y_pred = model_(X_test).argmax(axis=1)\n",
        "        Y = Y_test\n",
        "        test_recall_weighed, test_specificity_weighed, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=True)\n",
        "        test_recall_simple, test_specificity_simple, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=False)\n",
        "\n",
        "\n",
        "        if train_recall_weighed > Max_train_Acc: Max_train_Acc = train_recall_weighed\n",
        "        if valid_recall_weighed > Max_valid_Acc: Max_valid_Acc = valid_recall_weighed\n",
        "        if test_recall_weighed > Max_test_Acc: Max_test_Acc = test_recall_weighed\n",
        "        new_row = pd.Series({'train_recall_weighed':train_recall_weighed,'valid_recall_weighed':valid_recall_weighed,'test_recall_weighed':test_recall_weighed\n",
        "                                      ,'valid_recall_simple':valid_recall_simple,'test_recall_simple':test_recall_simple\n",
        "                                      ,'valid_specificity_weighed':valid_specificity_weighed,'test_specificity_weighed':test_specificity_weighed\n",
        "                                      ,'valid_specificity_simple':valid_specificity_simple,'test_specificity_simple':test_specificity_simple}, name='')\n",
        "        CV_Details = CV_Details.append(new_row)\n",
        "        pbar1.set_description(f'Max_train_Acc: {Max_train_Acc:.1%}, Max_valid_Acc: {Max_valid_Acc:.1%}, Max_test_Acc: {Max_test_Acc:.1%} ')\n",
        "        pbar1.set_postfix_str(f'Train_Acc: {train_recall_weighed:.1%} , Valide_Acc: {valid_recall_weighed:.1%} , Test_Acc: {test_recall_weighed:.1%}')\n",
        "\n",
        "\n",
        "        pickle.dump(CV_Details, open(G_path + '/08_Saved_Models_Outpus/Cross_Valid_Results/CV_280_Binary_'+ dat_hour,'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zKshOw9PaDt"
      },
      "outputs": [],
      "source": [
        "col_recall = ['test_recall_weighed','valid_recall_weighed']\n",
        "model_recalls = CV_Details[col_recall]\n",
        "col_specificity = ['test_specificity_weighed','valid_specificity_weighed']\n",
        "model_specificity = CV_Details[col_specificity]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB3G_Pkz6Il7"
      },
      "source": [
        "# **plot Test Specificity and Sensitivity box-plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYPZ9JOfPYIe"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(8,4))\n",
        "sns.boxplot(data=model_recalls)\n",
        "plt.title('Test performance of 100 Cross-Validation',fontsize = 18)\n",
        "plt.xlabel(\"Classifiers\", fontsize = 14)\n",
        "plt.ylabel(\"Accuracy\", fontsize = 14)\n",
        "plt.xticks(fontsize=16, rotation=0)\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(figsize=(8,4))\n",
        "sns.boxplot(data=model_specificity)\n",
        "plt.title('Valid performance of 100 Cross-Validation',fontsize = 18)\n",
        "plt.xlabel(\"Classifiers\", fontsize = 14)\n",
        "plt.ylabel(\"Accuracy\", fontsize = 14)\n",
        "plt.xticks(fontsize=16, rotation=0)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "CNN_280x210_Binary.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
