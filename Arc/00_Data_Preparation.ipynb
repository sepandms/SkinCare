{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_path = '/Users/sepehrbe/My_Drive/DataSources/SkinCare'\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from statistics import mean\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import pickle as pickle\n",
    "import PIL as pl\n",
    "from PIL.Image import Transpose\n",
    "from PIL import Image\n",
    "from usefull_functions import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Meta Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'HAM10000_metadata.csv'\n",
    "meta_data_v0 = pd.read_csv(G_path + '/archive/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplication\n",
    "meta_data_noDup = meta_data_v0.groupby('lesion_id').first().reset_index()\n",
    "# Encode Labels\n",
    "meta_data_noDup.dx = pd.Categorical(meta_data_noDup.dx)\n",
    "meta_data_noDup['image_label'] = meta_data_noDup.dx.cat.codes\n",
    "\n",
    "remap = {0:1 , 1:1, 4:1, 2:0, 3:0, 5:0, 6:0}\n",
    "meta_data_noDup['binary_label'] = meta_data_noDup['image_label']\n",
    "meta_data_noDup = meta_data_noDup.replace({'binary_label':remap})\n",
    "\n",
    "meta_data_noDup.to_csv(G_path + 'Meta_data_noDup.csv')\n",
    "### insert the code of R for Age filling\n",
    "\n",
    "######################\n",
    "filled_nan_ages = pd.read_csv(G_path+'/synthetics_ages.csv').rename(columns={'age':'age_mdf'})\n",
    "\n",
    "meta_data_noDup = meta_data_noDup.merge(filled_nan_ages,how='left', on='lesion_id')\n",
    "meta_data_noDup['age'] = meta_data_noDup.apply(lambda x: x.age if pd.notnull(x.age) else x.age_mdf, axis=1)\n",
    "\n",
    "try: meta_data_noDup.drop(columns=['Unnamed: 0','age_mdf'],inplace=True)\n",
    "except: None\n",
    "# Reduce Nr. class 5 category to 2800\n",
    "class_5 = meta_data_noDup[meta_data_noDup.image_label==5]\n",
    "class_5_reduced = class_5.sample(n=2000, random_state=0)\n",
    "meta_data_noDup_reduced = pd.concat([meta_data_noDup[meta_data_noDup.image_label!=5],class_5_reduced],axis=0)\n",
    "# Get \n",
    "train_meta, test_valid_meta = train_test_split(meta_data_noDup_reduced, test_size = 0.2, random_state=0,stratify=meta_data_noDup_reduced['image_label'])\n",
    "test_meta, valid_meta = train_test_split(test_valid_meta, test_size = 0.5, random_state=0,stratify=test_valid_meta['image_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.723293\n",
       "2    0.097323\n",
       "4    0.082195\n",
       "1    0.043775\n",
       "0    0.030522\n",
       "6    0.013119\n",
       "3    0.009772\n",
       "Name: image_label, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data_noDup.image_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.430385\n",
       "2    0.156445\n",
       "4    0.132128\n",
       "1    0.126748\n",
       "0    0.088229\n",
       "6    0.037874\n",
       "3    0.028190\n",
       "Name: image_label, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta_Data_Augmented.image_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta['type'] = 'train'\n",
    "valid_meta['type'] = 'valid'\n",
    "test_meta['type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "blc_test_meta = pd.concat([valid_meta,test_meta],axis=0)\n",
    "# blc_test_meta = blc_test_meta.groupby('image_label').head(15).reset_index()\n",
    "blc_test_meta = blc_test_meta.groupby('image_label').sample(n=15,random_state=0)\n",
    "pickle.dump(blc_test_meta , open(G_path + '/blc_test_meta_list','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "for path, dirs, files in  os.walk(G_path + '/archive'):\n",
    "    for f in files:\n",
    "        img_list.append( path +'/' + f)\n",
    "    for d in dirs:\n",
    "        img_list.append( path + d)\n",
    "img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Images for Augmentation Directory\n",
    "Augment_Path = G_path + '/' + 'Augmentation'\n",
    "try:\n",
    "    os.makedirs(Augment_Path)\n",
    "except:\n",
    "    None\n",
    "try:\n",
    "    os.makedirs(Augment_Path+'/test')\n",
    "    os.makedirs(Augment_Path+'/valid')\n",
    "    os.makedirs(Augment_Path+'/train')\n",
    "except:\n",
    "    None\n",
    "labels = list(train_meta.image_label.unique())\n",
    "\n",
    "for i in labels:\n",
    "    try:\n",
    "        os.makedirs(Augment_Path+'/test/'+str(i))\n",
    "        os.makedirs(Augment_Path+'/valid/'+str(i))\n",
    "        os.makedirs(Augment_Path+'/train/'+str(i))\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = train_meta[['image_id','image_label']]\n",
    "name = 'train'\n",
    "def copy_to_AutoEncoder(data_ , name):\n",
    "    id_list = list(data_['image_id'])\n",
    "    id_labels = list(data_['image_label'])\n",
    "    for i in range(len(id_list)):\n",
    "        for j in img_list:\n",
    "            if id_list[i] in j:\n",
    "                shutil.copy(j, Augment_Path+'/' + name+ '/' + str(id_labels[i]) + '/' + id_list[i] + '.jpg')\n",
    "                break\n",
    "\n",
    "data_ = train_meta[['image_id','image_label']]\n",
    "name = 'train'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)\n",
    "data_ = valid_meta[['image_id','image_label']]\n",
    "name = 'valid'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)\n",
    "data_ = test_meta[['image_id','image_label']]\n",
    "name = 'test'\n",
    "copy_to_AutoEncoder(data_ = data_ , name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = Augment_Path + '/train/'\n",
    "\n",
    "for i in [3, 6, 0, 1]:\n",
    "    f = folder_dir + str(i) + \"/\"\n",
    "    for image in os.listdir(f):\n",
    "        if \"ISIC\" in image and \"AUG_\" not in image and image.replace(\".jpg\", \"\"):\n",
    "            # print(\"Augmented image: \", image)\n",
    "            im1 = Image.open(f + image)\n",
    "            augmented = im1.transpose(Transpose.FLIP_LEFT_RIGHT)\n",
    "            augmented = augmented.transpose(Transpose.FLIP_TOP_BOTTOM)\n",
    "            augmented.save(folder_dir + \"/\" + str(i) + \"/AUG_\" + image)\n",
    "            # Add metadata of Augmented image\n",
    "            new_row = train_meta[train_meta.image_id == image.replace(\".jpg\", \"\")]\n",
    "            new_row['image_id'] = 'AUG_' + new_row['image_id'].iloc[0]\n",
    "            train_meta = train_meta.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta_Data_Augmented = pd.concat([train_meta, valid_meta, test_meta], axis=0)\n",
    "pickle.dump(Meta_Data_Augmented, open( G_path + '/Meta_Data_Augmented','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Augmented Images and rescaling to 280x210**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4647/4647 [01:31<00:00, 50.99it/s]\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "for path, dirs, files in  os.walk(Augment_Path):\n",
    "    for f in files:\n",
    "        img_list.append( path +'/' + f)\n",
    "    for d in dirs:\n",
    "        img_list.append( path + d)\n",
    "img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "Images_280x210 = G_path + '/Images_280x210'\n",
    "\n",
    "try:\n",
    "    os.makedirs(Images_280x210)\n",
    "except:\n",
    "    None\n",
    "\n",
    "for img in tqdm(img_list):\n",
    "    file_out = Images_280x210 +'/'+ re.findall('\\w+',img)[-2:-1][0] +'.jpg'\n",
    "    os.system('convert %s %s %s' % ('-resize 46.66%',img,file_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Rescaled Images and convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4648/4648 [00:09<00:00, 480.26it/s]\n"
     ]
    }
   ],
   "source": [
    "img_list = os.listdir(Images_280x210)\n",
    "Augmented_280x210_dataset = pd.DataFrame(columns=['image_id','img_array'])\n",
    "for img in tqdm(img_list):\n",
    "    if '.jpg' in img:\n",
    "        img_name = img.replace('.jpg','')\n",
    "        image = pl.Image.open( Images_280x210 +'/'+ img)\n",
    "        img_array = np.asarray(image)\n",
    "        new_row = pd.Series({'image_id':img_name,'img_array':img_array}, name='')\n",
    "        Augmented_280x210_dataset = Augmented_280x210_dataset.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_280x210_dataset = Augmented_280x210_dataset.merge(Meta_Data_Augmented, how=\"left\",on='image_id')\n",
    "blc_test_dataset_280x210 = Augmented_280x210_dataset[['image_id','img_array']].merge(blc_test_meta, how=\"right\",on='image_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN_280x210: Standardization / Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3833  Valid Size:  407  Test size:  407\n"
     ]
    }
   ],
   "source": [
    "df_ = CNN_280x210_dataset\n",
    "train_data = df_[df_.type=='train']\n",
    "valid_data = df_[df_.type=='valid']\n",
    "test_data = df_[df_.type=='test']\n",
    "print('Train size: ',train_data.shape[0] ,' Valid Size: ',valid_data.shape[0], ' Test size: ', test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.76570269 0.55224556 0.57657015]   STD: [0.13616887 0.1481389  0.16343116]\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.concat([train_data,valid_data], axis=0)\n",
    "training_arrays_scaled = np.stack(training_data['img_array'].values)  / 255\n",
    "Mean = training_arrays_scaled.mean(axis = (0,1,2)) \n",
    "STD = training_arrays_scaled.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean}   STD: {STD}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data standardization\n",
    "train_arrays_std = np.stack(train_data['img_array'].values) / 255\n",
    "for i in range(0,train_arrays_std.shape[0]):\n",
    "    train_arrays_std[i] = (train_arrays_std[i] - Mean) / STD\n",
    "#Validation data standardization\n",
    "valid_arrays_std = np.stack(valid_data['img_array'].values) / 255\n",
    "for i in range(0,valid_arrays_std.shape[0]):\n",
    "    valid_arrays_std[i] = (valid_arrays_std[i] - Mean) / STD\n",
    "#Test data standardization\n",
    "test_arrays_std = np.stack(test_data['img_array'].values) / 255\n",
    "for i in range(0,test_arrays_std.shape[0]):\n",
    "    test_arrays_std[i] = (test_arrays_std[i] - Mean) / STD\n",
    "\n",
    "#Test-Valid data standardization\n",
    "test_valid_arrays_std = np.stack(blc_test_dataset_280x210['img_array'].values) / 255\n",
    "for i in range(0,test_valid_arrays_std.shape[0]):\n",
    "    test_valid_arrays_std[i] = (test_valid_arrays_std[i] - Mean) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean : [-0.01839491 -0.04231122 -0.04687724]   Test Set STD: [0.96394774 0.99849143 1.00558878]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = test_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = test_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Test Set Mean : {Mean_t}   Test Set STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metda_keys = train_data[['type','image_id']].reset_index(drop=True), valid_data[['type','image_id']].reset_index(drop=True), test_data[['type','image_id']].reset_index(drop=True)\n",
    "\n",
    "train_arrays_std_T = torch.tensor(train_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "train_labels = torch.tensor(train_data['image_label'].values.astype(np.long))\n",
    "\n",
    "valid_arrays_std_T = torch.tensor(valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "valid_labels = torch.tensor(valid_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_valid_arrays_std_T = torch.tensor(test_valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_valid_labels = torch.tensor(blc_test_dataset_280x210['image_label'].values.astype(np.long))\n",
    "\n",
    "train_labels_binary =  torch.tensor(label_to_binary(train_labels))\n",
    "valid_labels_binary =  torch.tensor(label_to_binary(valid_labels))\n",
    "test_labels_binary =  torch.tensor(label_to_binary(test_labels))\n",
    "test_valid_labels_binary =  torch.tensor(label_to_binary(test_valid_labels))\n",
    "\n",
    "\n",
    "input_data = (train_arrays_std_T , valid_arrays_std_T, test_arrays_std_T ,metda_keys)\n",
    "labels = ((train_labels,valid_labels,test_labels), (train_labels_binary , valid_labels_binary ,test_labels_binary))\n",
    "\n",
    "balc_evaluation_set = (test_valid_arrays_std_T,test_valid_labels,test_valid_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = G_path + '/DataSet_280x210'\n",
    "\n",
    "try:\n",
    "    os.makedirs(dataset_training)\n",
    "except:\n",
    "    None\n",
    "\n",
    "pickle.dump((input_data,labels), open(dataset_training + '/DataSet_280x210', 'wb'))\n",
    "pickle.dump(balc_evaluation_set, open(dataset_training + '/Evaluation_set_280x210', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import AutoEncoders Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3833/3833 [00:05<00:00, 739.10it/s]\n",
      "100%|██████████| 407/407 [00:00<00:00, 748.03it/s]\n",
      "100%|██████████| 407/407 [00:00<00:00, 762.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def import_imges(img_path):\n",
    "    img_list = []\n",
    "    for path, dirs, files in  os.walk(img_path):\n",
    "        for f in files:\n",
    "            img_list.append( path +'/' + f)\n",
    "        for d in dirs:\n",
    "            img_list.append( path + d)\n",
    "    img_list = list(set([x for x in img_list if \".jpg\" in x] ))\n",
    "\n",
    "    dataset = pd.DataFrame(columns=['image_id','img_array'])\n",
    "    for img in tqdm(img_list):\n",
    "        img_name = re.findall('\\w+', img)[-2:-1][0]\n",
    "        image = pl.Image.open( img)\n",
    "        img_array = np.asarray(image)\n",
    "        new_row = pd.Series({'image_id':img_name,'img_array':img_array}, name='')\n",
    "        dataset = dataset.append(new_row)\n",
    "    return dataset\n",
    "\n",
    "train_path = G_path + '/Augmented_AutoEncoded/train'\n",
    "valid_path = G_path + '/Augmented_AutoEncoded/valid'\n",
    "test_path = G_path + '/Augmented_AutoEncoded/test'\n",
    "\n",
    "train_set = import_imges(train_path)\n",
    "valid_set = import_imges(valid_path)\n",
    "test_set = import_imges(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = pd.concat([train_set,valid_set,test_set],axis=0)\n",
    "A_E_dataset = dataset_.merge(Meta_Data_Augmented, how='left', on='image_id')\n",
    "blc_test_dataset_A_E = dataset_[['image_id','img_array']].merge(blc_test_meta, how=\"right\",on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  3833  Valid Size:  407  Test size:  407\n"
     ]
    }
   ],
   "source": [
    "df_ = A_E_dataset\n",
    "train_data = df_[df_.type=='train']\n",
    "valid_data = df_[df_.type=='valid']\n",
    "test_data = df_[df_.type=='test']\n",
    "print('Train size: ',train_data.shape[0] ,' Valid Size: ',valid_data.shape[0], ' Test size: ', test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [0.36115692 0.34408239 0.32405245]   STD: [0.16846211 0.26209011 0.24330585]\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.concat([train_data,valid_data], axis=0)\n",
    "training_arrays_scaled = np.stack(training_data['img_array'].values)  / 255\n",
    "Mean = training_arrays_scaled.mean(axis = (0,1,2)) \n",
    "STD = training_arrays_scaled.std(axis = (0,1,2))\n",
    "print(f\"Mean : {Mean}   STD: {STD}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data standardization\n",
    "train_arrays_std = np.stack(train_data['img_array'].values) / 255\n",
    "for i in range(0,train_arrays_std.shape[0]):\n",
    "    train_arrays_std[i] = (train_arrays_std[i] - Mean) / STD\n",
    "#Validation data standardization\n",
    "valid_arrays_std = np.stack(valid_data['img_array'].values) / 255\n",
    "for i in range(0,valid_arrays_std.shape[0]):\n",
    "    valid_arrays_std[i] = (valid_arrays_std[i] - Mean) / STD\n",
    "#Test data standardization\n",
    "test_arrays_std = np.stack(test_data['img_array'].values) / 255\n",
    "for i in range(0,test_arrays_std.shape[0]):\n",
    "    test_arrays_std[i] = (test_arrays_std[i] - Mean) / STD\n",
    "\n",
    "#Test-Valid data standardization\n",
    "test_valid_arrays_std = np.stack(blc_test_dataset_A_E['img_array'].values) / 255\n",
    "for i in range(0,test_valid_arrays_std.shape[0]):\n",
    "    test_valid_arrays_std[i] = (test_valid_arrays_std[i] - Mean) / STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean : [-0.04389305 -0.05144053  0.03501551]   Test Set STD: [0.93557654 0.98728975 1.00074642]\n"
     ]
    }
   ],
   "source": [
    "Mean_t = test_arrays_std.mean(axis = (0,1,2)) \n",
    "STD_t = test_arrays_std.std(axis = (0,1,2))\n",
    "print(f\"Test Set Mean : {Mean_t}   Test Set STD: {STD_t}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metda_keys = train_data[['type','image_id']].reset_index(drop=True), valid_data[['type','image_id']].reset_index(drop=True), test_data[['type','image_id']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays_std_T = torch.tensor(train_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "train_labels = torch.tensor(train_data['image_label'].values.astype(np.long))\n",
    "\n",
    "valid_arrays_std_T = torch.tensor(valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "valid_labels = torch.tensor(valid_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_arrays_std_T = torch.tensor(test_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_labels = torch.tensor(test_data['image_label'].values.astype(np.long))\n",
    "\n",
    "test_valid_arrays_std_T = torch.tensor(test_valid_arrays_std, dtype= torch.float32).transpose(3,1)\n",
    "test_valid_labels = torch.tensor(blc_test_dataset_A_E['image_label'].values.astype(np.long))\n",
    "\n",
    "A_E_input_data = (train_arrays_std_T , valid_arrays_std_T, test_arrays_std_T ,metda_keys)\n",
    "\n",
    "train_labels_binary =  torch.tensor(label_to_binary(train_labels))\n",
    "valid_labels_binary =  torch.tensor(label_to_binary(valid_labels))\n",
    "test_labels_binary =  torch.tensor(label_to_binary(test_labels))\n",
    "\n",
    "A_E_labels = ((train_labels,valid_labels,test_labels), (train_labels_binary , valid_labels_binary ,test_labels_binary))\n",
    "\n",
    "A_E_balc_test_set = (test_valid_arrays_std_T,test_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((A_E_input_data,A_E_labels), open(G_path + '/Augmented_AutoEncoded/'+ 'Latent_Input_Data_labels', 'wb'))\n",
    "pickle.dump(A_E_balc_test_set, open(G_path +  '/Augmented_AutoEncoded/'+  '/Evaluation_set_AutoEncoder', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
