{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  CWD = '/content/drive/MyDrive/DataSources/SkinCare'\n",
    "  os.chdir(CWD)\n",
    "except:None\n",
    "G_path = './Project_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devis: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#import torchvision.transforms.Compose\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import features\n",
    "from features.usefull_functions import *\n",
    "from features.NETs import *\n",
    "from features.Model_Training import *\n",
    "device = None\n",
    "try:\n",
    "    c = torch.cuda.is_available()\n",
    "    if c:\n",
    "        print('devis: cuda')\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        try :\n",
    "            m = torch.backends.mps.is_available()\n",
    "            if m:\n",
    "                device = 'mps'\n",
    "                print('devis: mps')  \n",
    "        except:    \n",
    "            device = 'cpu'\n",
    "            print('devis: cpu')           \n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import DataSets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Input_DataSet_360x270' # [Input_DataSet_280x210,Input_DataSet_360x270]\n",
    "input_data , labels = pickle.load(open(G_path + '/06_Rescaled_DataSet/'+ file,'rb'))\n",
    "labels_biary = labels[1]\n",
    "labels_multi = labels[0]\n",
    "\n",
    "file = 'Input_DataSet_A_E' \n",
    "A_E_input_data , A_E_labels = pickle.load(open(G_path + '/05_AutoEncoded_DataSet/'+ file,'rb'))\n",
    "A_E_labels_biary = A_E_labels[1]\n",
    "A_E_labels_multi = A_E_labels[0]\n",
    "\n",
    "file = 'Balanced_Test_Set_360x270' \n",
    "BLC_test = pickle.load(open(G_path + '/06_Rescaled_DataSet/' + file,'rb'))\n",
    "\n",
    "file = 'Balanced_Test_Set_A_E' \n",
    "BLC_test_A_E = pickle.load(open(G_path + '/05_AutoEncoded_DataSet/' + file,'rb'))\n",
    "\n",
    "Meta_Data_Augmented = pickle.load(open( G_path + '/02_Augmented_MetaData/Meta_Data_Augmented','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_280 = 'CNN_280x210_MultiClassModel0_24_Jun22_22-38' # Best of grid search ['CNN_280x210_Model5_23_Jun22_12-55','CNN_280x210_Model4_22_Jun22_00-14]\n",
    "C_280 = 'CNN_280x210_MultiClassModel1_12_Jul22_18_15'\n",
    "C_280 = 'CNN_280x210_MultiClassModel41_14_Jul22_10_49'\n",
    "C_280 = 'CNN_360x270_MultiClassModel14_15_Jul22_10_05'\n",
    "C_280_binary = 'CNN_360x270_Binary_Model11_21_Jul22_06_50' #[CNN_360x270_Binary_Model11_21_Jul22_06_50,CNN_360x270_BinaryModel1_20_Jul22_12_45]\n",
    "\n",
    "Model_multi = pickle.load(open(G_path + '/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/'+ C_280 , 'rb') ).eval()\n",
    "Model_binary = pickle.load(open(G_path + '/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/'+ C_280_binary , 'rb') ).eval()\n",
    "\n",
    "A_E = 'A_E_CNN_Model6_22_Jun22_00-29' #Best of grid search\n",
    "A_E_binary = 'A_E_CNN_Model0_25_Jun22_09-35'\n",
    "# [CNN_Nets.Net8_a, Model_Training_with_loader] = pickle.load(open(G_Models+'/Net8_Model_Trainin','rb'))\n",
    "Model_A_En_multiclass = pickle.load(open(G_path + '/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' + A_E , 'rb') ).eval()\n",
    "Model_A_En_binary = pickle.load(open(G_path + '/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' + A_E_binary , 'rb') ).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of Data : \n",
      " 407\n",
      "Accuracy of The Model : \n",
      " 0.8255528255528255\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_297ee_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >weights</th>\n",
       "      <th class=\"col_heading level0 col1\" >Precision</th>\n",
       "      <th class=\"col_heading level0 col2\" >Recall_Sensitivity</th>\n",
       "      <th class=\"col_heading level0 col3\" >Specificity</th>\n",
       "      <th class=\"col_heading level0 col4\" >f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_297ee_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_297ee_row0_col0\" class=\"data row0 col0\" >71.3%</td>\n",
       "      <td id=\"T_297ee_row0_col1\" class=\"data row0 col1\" >84.5%</td>\n",
       "      <td id=\"T_297ee_row0_col2\" class=\"data row0 col2\" >92.4%</td>\n",
       "      <td id=\"T_297ee_row0_col3\" class=\"data row0 col3\" >58.1%</td>\n",
       "      <td id=\"T_297ee_row0_col4\" class=\"data row0 col4\" >88.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_297ee_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_297ee_row1_col0\" class=\"data row1 col0\" >28.7%</td>\n",
       "      <td id=\"T_297ee_row1_col1\" class=\"data row1 col1\" >75.6%</td>\n",
       "      <td id=\"T_297ee_row1_col2\" class=\"data row1 col2\" >58.1%</td>\n",
       "      <td id=\"T_297ee_row1_col3\" class=\"data row1 col3\" >92.4%</td>\n",
       "      <td id=\"T_297ee_row1_col4\" class=\"data row1 col4\" >65.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_297ee_level0_row2\" class=\"row_heading level0 row2\" >Simple Avg.</th>\n",
       "      <td id=\"T_297ee_row2_col0\" class=\"data row2 col0\" >nan%</td>\n",
       "      <td id=\"T_297ee_row2_col1\" class=\"data row2 col1\" >80.0%</td>\n",
       "      <td id=\"T_297ee_row2_col2\" class=\"data row2 col2\" >75.3%</td>\n",
       "      <td id=\"T_297ee_row2_col3\" class=\"data row2 col3\" >75.3%</td>\n",
       "      <td id=\"T_297ee_row2_col4\" class=\"data row2 col4\" >77.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_297ee_level0_row3\" class=\"row_heading level0 row3\" >Weighted Avg.</th>\n",
       "      <td id=\"T_297ee_row3_col0\" class=\"data row3 col0\" >nan%</td>\n",
       "      <td id=\"T_297ee_row3_col1\" class=\"data row3 col1\" >82.0%</td>\n",
       "      <td id=\"T_297ee_row3_col2\" class=\"data row3 col2\" >82.6%</td>\n",
       "      <td id=\"T_297ee_row3_col3\" class=\"data row3 col3\" >68.0%</td>\n",
       "      <td id=\"T_297ee_row3_col4\" class=\"data row3 col4\" >81.8%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x29f023bb0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 280x210\n",
    "Evaluation_set = 2  #{train:0 , valid:1 , test:2}\n",
    "balanced_test_set = False\n",
    "binary_cnn = True\n",
    "Multi_to_binary = False\n",
    "model = Model_binary  # {Model_multi, CNN_A_En_multiclass}\n",
    "\n",
    "if balanced_test_set:\n",
    "    X = BLC_test[0]\n",
    "    if binary_cnn:\n",
    "        Y = BLC_test[2]\n",
    "    else:\n",
    "        Y = BLC_test[1]      \n",
    "else:\n",
    "    X = input_data[Evaluation_set]\n",
    "    if binary_cnn:\n",
    "        Y = labels_biary[Evaluation_set]\n",
    "    else:\n",
    "        Y = labels_multi[Evaluation_set]\n",
    "n = 100\n",
    "iter = X.shape[0] // n\n",
    "Y_pred = torch.zeros(0)\n",
    "Y_pred_prob = torch.zeros(0)\n",
    "for i in range(iter+1):\n",
    "    X_ = X[i*n:(i+1)*n].detach()\n",
    "    Y_prob = torch.softmax(model.forward_noDrop(X_),dim=1).detach()\n",
    "    Y_ = model.forward_noDrop(X_).argmax(dim=1).detach()\n",
    "    Y_pred = torch.cat([Y_pred,Y_]).detach()\n",
    "    Y_pred_prob = torch.cat([Y_pred_prob,Y_prob]).detach()\n",
    "\n",
    "if Multi_to_binary:\n",
    "    Y_binary =  label_to_binary(Y)\n",
    "    Y_pred_binary =  label_to_binary(Y_pred)\n",
    "    results = model_evaluation(Y_binary,Y_pred_binary)\n",
    "else:\n",
    "    results = model_evaluation(Y,Y_pred,'CNN')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN: MultiClass/Binary: Get CNN OutPut (Soft-Max)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5807, 3, 360, 270]), torch.Size([5807, 2]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apply_SoftMax = True\n",
    "\n",
    "model_ = Model_binary #[Model_multi, Model_binary]\n",
    "X = torch.cat([input_data[0],input_data[1],input_data[2]],dim=0).detach()\n",
    "n = 100\n",
    "iter = X.shape[0] // n\n",
    "Y_pred = torch.zeros(0)\n",
    "for i in range(iter+1):\n",
    "    X_ = X[i*n:(i+1)*n]\n",
    "    if Apply_SoftMax:\n",
    "        Y_ = torch.softmax(model_.forward_noDrop(X_),dim=1).detach()\n",
    "    else:\n",
    "    # no SoftMax\n",
    "        Y_ = model_.forward_noDrop(X_).detach()\n",
    "    Y_pred = torch.cat([Y_pred,Y_]).detach()\n",
    "X.shape , Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0000, 1.0000]),\n",
       "indices=tensor([  87, 3397]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMax_Scaling(T):\n",
    "    T_S = torch.zeros(0)\n",
    "    for i in range(T.shape[0]):\n",
    "        T_ = T[i:i+1]\n",
    "        min = T_.min()\n",
    "        max = T_.max()\n",
    "        T_ = (T_ - min) / (max-min)\n",
    "        T_S = torch.cat([T_S,T_]).detach()\n",
    "    return T_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply_scaling = True\n",
    "if Apply_scaling:\n",
    "    Y_pred = MinMax_Scaling(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN = pd.DataFrame(Y_pred.detach().numpy()).add_prefix('CNN_')\n",
    "meta_data_key = pd.concat([input_data[3][0],input_data[3][1],input_data[3][2]],axis=0).reset_index(drop=True)\n",
    "df_CNN = pd.concat([meta_data_key,df_CNN],axis=1).reset_index(drop=True)\n",
    "CNN_280x210_data = df_CNN.merge(Meta_Data_Augmented.drop(columns=['type']) ,how='left', on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN_280x210_data.sample(100).drop(columns=['lesion_id','dx']).style.format({'CNN_0': \"{:.1%}\",'CNN_1': \"{:.1%}\",'CNN_2': \"{:.1%}\",'CNN_3': \"{:.1%}\",'CNN_4': \"{:.1%}\",'CNN_5': \"{:.1%}\",'CNN_6': \"{:.1%}\",'age': \"{:0}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['localization','sex','dx_type']\n",
    "CNN_280x210_data_encoded = pd.get_dummies(CNN_280x210_data, columns= cols).reset_index(drop=True)\n",
    "def standardizaion(dataset, attr):\n",
    "    mean = dataset[dataset.type.str.contains('train')][attr].mean()\n",
    "    std = dataset[dataset.type.str.contains('train')][attr].std()\n",
    "    dataset[attr] = (dataset[attr] - mean ) / std\n",
    "    return dataset\n",
    "CNN_280x210_data_encoded_STD = standardizaion(CNN_280x210_data_encoded,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(CNN_280x210_data_encoded_STD, open(G_path + '/07_CNNs_Output_DataSet/'+ 'CNN_360x270_Ouput_Binary_SoftMax', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN: AutoEncoder, Get CNN OutPut (Soft-Max)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4647, 3, 282, 207]), torch.Size([4647, 7]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = Model_A_En_multiclass\n",
    "X = torch.cat([A_E_input_data[0],A_E_input_data[1],A_E_input_data[2]],dim=0).detach()\n",
    "n = 100\n",
    "iter = X.shape[0] // n\n",
    "Y_pred = torch.zeros(0)\n",
    "for i in range(iter+1):\n",
    "    X_ = X[i*n:(i+1)*n]\n",
    "    Y_ = torch.softmax(model_.forward_noDrop(X_),dim=1).detach()\n",
    "    Y_pred = torch.cat([Y_pred,Y_]).detach()\n",
    "X.shape , Y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN = pd.DataFrame(Y_pred.detach().numpy()).add_prefix('CNN_')\n",
    "meta_data_key = pd.concat([A_E_input_data[3][0],A_E_input_data[3][1],A_E_input_data[3][2]],axis=0).reset_index(drop=True)\n",
    "df_CNN = pd.concat([meta_data_key,df_CNN],axis=1).reset_index(drop=True)\n",
    "CNN_A_E_data = df_CNN.merge(Meta_Data_Augmented.drop(columns=['type']) ,how='left', on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['localization','sex','dx_type']\n",
    "CNN_A_E_data_encoded = pd.get_dummies(CNN_A_E_data, columns= cols).reset_index(drop=True)\n",
    "def standardizaion(dataset, attr):\n",
    "    mean = dataset[dataset.type.str.contains('train')][attr].mean()\n",
    "    std = dataset[dataset.type.str.contains('train')][attr].std()\n",
    "    dataset[attr] = (dataset[attr] - mean ) / std\n",
    "    return dataset\n",
    "CNN_A_E_data_encoded_STD = standardizaion(CNN_A_E_data_encoded,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(CNN_A_E_data_encoded_STD, open(G_path + '/07_CNNs_Output_DataSet/'+ 'CNN_A_E_Ouput_MultiClass', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN: AutoEncoder Binary, Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4647, 3, 282, 207]), torch.Size([4647, 2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = Model_A_En_binary\n",
    "X = torch.cat([A_E_input_data[0],A_E_input_data[1],A_E_input_data[2]],dim=0).detach()\n",
    "n = 100\n",
    "iter = X.shape[0] // n\n",
    "Y_pred = torch.zeros(0)\n",
    "for i in range(iter+1):\n",
    "    X_ = X[i*n:(i+1)*n]\n",
    "    Y_ = torch.softmax(model_.forward_noDrop(X_),dim=1).detach()\n",
    "    Y_pred = torch.cat([Y_pred,Y_]).detach()\n",
    "X.shape , Y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN = pd.DataFrame(Y_pred.detach().numpy()).add_prefix('CNN_')\n",
    "meta_data_key = pd.concat([A_E_input_data[3][0],A_E_input_data[3][1],A_E_input_data[3][2]],axis=0).reset_index(drop=True)\n",
    "df_CNN = pd.concat([meta_data_key,df_CNN],axis=1).reset_index(drop=True)\n",
    "CNN_A_E_data = df_CNN.merge(Meta_Data_Augmented.drop(columns=['type']) ,how='left', on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['localization','sex','dx_type']\n",
    "CNN_A_E_data_encoded = pd.get_dummies(CNN_A_E_data, columns= cols).reset_index(drop=True)\n",
    "def standardizaion(dataset, attr):\n",
    "    mean = dataset[dataset.type.str.contains('train')][attr].mean()\n",
    "    std = dataset[dataset.type.str.contains('train')][attr].std()\n",
    "    dataset[attr] = (dataset[attr] - mean ) / std\n",
    "    return dataset\n",
    "CNN_A_E_data_encoded_STD = standardizaion(CNN_A_E_data_encoded,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(CNN_A_E_data_encoded_STD, open(G_path + '/07_CNNs_Output_DataSet/'+ 'CNN_A_E_Ouput_Binary', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
