{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25489,"status":"ok","timestamp":1657551473256,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"},"user_tz":-120},"id":"MzRoZIkXDiT7","outputId":"c79be73e-55e0-463c-9df9-4977dd05b9fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  import os\n","  CWD = '/content/drive/MyDrive/DataSources/SkinCare'\n","  os.chdir(CWD)\n","except:None\n","G_path = './Project_Data'"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5824,"status":"ok","timestamp":1657551479074,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"},"user_tz":-120},"id":"VZrZXitoAlDd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"99332317-93ab-4033-ea11-a86594396c8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["devis: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset as dset\n","#import torchvision.transforms.Compose\n","import numpy as np\n","from datetime import datetime\t\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.utils.data import random_split\n","from torch.utils.data import SubsetRandomSampler\n","from torch.utils.data import WeightedRandomSampler\n","from torch.utils.data import DataLoader\n","import time\n","from tqdm import tqdm\n","import pickle as pickle\n","from statistics import mean\n","import pandas as pd\n","pd.options.display.max_colwidth = 250\n","import sklearn as sk\n","from sklearn.model_selection import train_test_split\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n","from sklearn.utils import resample\n","import warnings\n","warnings.filterwarnings('ignore')\n","import features\n","from features.usefull_functions import *\n","from features.NETs import *\n","from features.Model_Training import *\n","device = None\n","try:\n","    c = torch.cuda.is_available()\n","    if c:\n","        print('devis: cuda')\n","        device = 'cuda'\n","    else:\n","        try :\n","            m = torch.backends.mps.is_available()\n","            if m:\n","                device = 'mps'\n","                print('devis: mps')  \n","        except:    \n","            device = 'cpu'\n","            print('devis: cpu')           \n","except:\n","    None"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":66907,"status":"ok","timestamp":1657551545976,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"},"user_tz":-120},"id":"6y2NEQ6WOvNi"},"outputs":[],"source":["Binary_classification = False\n","file = 'Input_DataSet_280x210' \n","input_data , labels = pickle.load(open(G_path + '/06_Rescaled_DataSet/'+ file,'rb'))\n","file = 'Balanced_Test_Set_280x210' \n","Evaluation_set = pickle.load(open(G_path + '/06_Rescaled_DataSet/' + file,'rb'))\n","\n","if Binary_classification:\n","  labels = labels[1]\n","else:\n","  labels = labels[0]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_Mz7iN2hFgF","outputId":"40b17006-a0d6-4508-f9c5-b3b7cc6a4481","executionInfo":{"status":"ok","timestamp":1657551545976,"user_tz":-120,"elapsed":7,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5, 5, 4, 5, 2, 1, 5, 2, 5, 5])"]},"metadata":{},"execution_count":7}],"source":["labels[1][:10]"]},{"cell_type":"markdown","metadata":{"id":"YX40UaIYFt0J"},"source":["# **CNN Networks: Configuration**"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HghWL1MnzLFX","executionInfo":{"status":"ok","timestamp":1657551549892,"user_tz":-120,"elapsed":3920,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}},"outputId":"b6940175-730a-444e-ecd9-00599562e0c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net1\n","202752\n","Net10\n","4480\n","Net11\n","1080\n","Net2\n","221952\n","Net3\n","12288\n","Net4\n","12288\n","Net5\n","56304\n","Net6\n","55488\n","Net7\n","384\n","Net8\n","384\n","Net8_a\n","384\n","Net8_a_binary\n","1536\n","Net8_b\n","1536\n","Net8_b_binary\n","1536\n","Net9\n","384\n"]}],"source":["net_list = [func for func in dir(CNN_Nets) if callable(getattr(CNN_Nets, func)) and not func.startswith(\"__\")]\n","X_ = input_data[0][0:5]\n","for net in net_list:\n","    print(net)\n","    model_ = getattr(CNN_Nets,net)\n","    fc_features = model_().dimention_set(X_)\n","    print(fc_features)\n","    model_ = getattr(CNN_Nets,net)\n","    model_.fc_features = fc_features\n","    out = model_().forward(X_)\n","    setattr(CNN_Nets,net,model_)\n","    # print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"XWC5a989AlDg"},"source":["# **Metrics and Performance**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pBE96mxDAlDg","executionInfo":{"status":"ok","timestamp":1657440314691,"user_tz":-120,"elapsed":481,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[],"source":["\n","\n","def plot_loss_accuracy(model_):\n","    epochs_X = [i for i in range(1, len(model_.Epochs_Train_loss)+1)]\n","    fig, axs = plt.subplots(1,2,figsize=(14,4))\n","    axs[0].plot(epochs_X , model_.Epochs_Train_loss , 'bo-', label='Train loss')\n","    axs[0].plot(epochs_X , model_.Epochs_Val_loss,'ro-', label='Validation loss')\n","    axs[0].plot(epochs_X , model_.Epochs_test_loss,'go-', label='Test loss')\n","    axs[0].set_xlabel(\"Epochs\", fontsize = 12)\n","    axs[0].set_ylabel(\"Loss\", fontsize = 12)\n","    axs[0].grid()\n","    axs[0].legend()\n","    axs[0].set_title('Train and Validation loss by epochs', fontsize = 14)\n","    axs[1].plot(epochs_X , model_.Epochs_Train_Acc , 'bo-', label='Train Accuracy')\n","    axs[1].plot(epochs_X , model_.Epochs_Val_Acc ,'ro-', label='Validation Accuracy')\n","    axs[1].plot(epochs_X , model_.Epochs_test_Acc ,'go-', label='Test Accuracy')\n","    axs[1].set_xlabel(\"Epochs\", fontsize = 12)\n","    axs[1].set_ylabel(\"Accuracy\", fontsize = 12)\n","    axs[1].grid()\n","    axs[1].legend()\n","    axs[1].set_title('Train and Validation Accuracy by epochs', fontsize = 14)\n","    plt.show()\n","\n","def recall_specificity_precision(Y,Y_pred, weighted_avg):\n","    CM = sk.metrics.confusion_matrix(Y,Y_pred)\n","    FP = CM.sum(axis=0) - np.diag(CM) \n","    FN = CM.sum(axis=1) - np.diag(CM)\n","    TP = np.diag(CM)\n","    TN = CM.sum() - (FP + FN + TP)\n","    weights = CM.sum(axis=1) / CM.sum() \n","    Accuracy = np.nan_to_num((TP+TN)/(TP+FP+FN+TN) , nan=0)\n","    Recall = np.nan_to_num(TP/(TP+FN) , nan=0)\n","    Specificity = np.nan_to_num(TN/(TN+FP) , nan=0)\n","    Precision = np.nan_to_num(TP/(TP+FP) , nan=0)\n","    if weighted_avg:\n","        return round(sum(weights*Recall),3), round(sum(weights*Specificity),3), round(sum(weights*Precision),3)\n","    else:\n","        return round(mean(Recall),3), round(mean(Specificity),3), round(mean(Precision),3)"]},{"cell_type":"markdown","metadata":{"id":"0w-7sG3Slop1"},"source":["# **Define Training data**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"sM2zFYZBWR45","executionInfo":{"status":"ok","timestamp":1657551550577,"user_tz":-120,"elapsed":688,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[],"source":["X_train = input_data[0]\n","Y_train = labels[0]\n","X_valid = input_data[1]\n","Y_valid = labels[1]\n","X_test = input_data[2]\n","Y_test = labels[2]\n","\n","\n","label_freq = np.bincount(Y_train)\n","labels_weights = 1. / label_freq\n","weights = labels_weights[Y_train]\n","w_sampler = WeightedRandomSampler(weights, len(weights))\n","\n","trainDataset = dset(X_train, Y_train)\n","validDataset = dset(X_valid, Y_valid)\n","testDataset = dset(X_test, Y_test)"]},{"cell_type":"markdown","metadata":{"id":"geKyTRJ8AlDh"},"source":["# **Model by Grid**"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VL5FxPLAAlDi","executionInfo":{"status":"ok","timestamp":1657551555737,"user_tz":-120,"elapsed":619,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[],"source":["\n","Net = [CNN_Nets.Net8_b]\n","Drop = [0.25]\n","LR = [ 1.1e-3]\n","batch_size = [24]\n","Momentum = [0.77]\n","epochs = [2]\n","patience = [20]\n","weight_decay = [1e-3]\n","loss_func  =  [nn.CrossEntropyLoss]\n","opt_func = [torch.optim.SGD]\n","\n","\n","grid = {\n","    'Net' : Net\n","    ,'Drop' : Drop\n","    ,'LR' : LR\n","    ,'batch_size' : batch_size\n","    ,'Momentum' : Momentum\n","    ,'epochs' : epochs\n","    ,'patience': patience\n","    ,'weight_decay' :weight_decay\n","    ,'loss_func'  :loss_func\n","    ,'opt_func' : opt_func\n","}\n","params = sk.model_selection.ParameterGrid(grid)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEkHc_7xAlDi","outputId":"155e206a-47fb-47ae-aba8-f3fabb1dbde7","executionInfo":{"status":"ok","timestamp":1657384615521,"user_tz":-120,"elapsed":42599,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Epoch: 1]  , Train_loss: 1.9 , Train_Acc: 17.3%, Val_loss: 1.8 , Val_Acc: 41.7%, Test_Acc: 36.8%  , run time: 6.1\n","[Epoch: 2]  , Train_loss: 1.8 , Train_Acc: 26.4%, Val_loss: 1.7 , Val_Acc: 40.0%, Test_Acc: 35.0%  , run time: 6.02\n"]},{"output_type":"stream","name":"stderr","text":["Max_Test_Recall: 27.3%, Max_BLC_Test_Recall: 25.7%, Max_Tr_Acc: 26.4%, Max_V_Acc: 40.0%, Max_Te_Acc: 35.1% :  50%|█████     | 1/2 [00:20<00:20, 20.31s/it, Train_Acc: 26.4% , Valide_Acc: 40.0% , Test_Acc: 35.1%]"]},{"output_type":"stream","name":"stdout","text":["[Epoch: 1]  , Train_loss: 1.9 , Train_Acc: 16.6%, Val_loss: 1.8 , Val_Acc: 43.6%, Test_Acc: 39.0%  , run time: 6.59\n","[Epoch: 2]  , Train_loss: 1.8 , Train_Acc: 24.6%, Val_loss: 1.7 , Val_Acc: 40.7%, Test_Acc: 36.5%  , run time: 6.62\n"]},{"output_type":"stream","name":"stderr","text":["Max_Test_Recall: 27.5%, Max_BLC_Test_Recall: 27.6%, Max_Tr_Acc: 26.4%, Max_V_Acc: 40.8%, Max_Te_Acc: 36.6% : 100%|██████████| 2/2 [00:42<00:00, 21.07s/it, Train_Acc: 24.6% , Valide_Acc: 40.8% , Test_Acc: 36.6%]\n"]}],"source":["print_epochs = True\n","save_results = False\n","save_models = False\n","cross_valid = False\n","nr_repeat =1\n","\n","export_name = 'CNN_280x210_MultiClass' # {'280_210_','AutoEncoder_'}\n","\n","Hyper_Details = pd.DataFrame(columns=['model_name','hyper_param','test_accuracy','valid_accuracy','train_accuracy','test_specificity','valid_specificity'\n","                                  ,'train_epoch_loss','train_epoch_acc','valid_epoch_loss','valid_epoch_acc','test_epoch_acc','test_epoch_loss','test_fpr','test_tpr','test_roc_auc','train_index','valid_index'])\n","date_hour = datetime.now().strftime(\"%d_%b%y_%H-%M\") \n","Max_train_Acc = 0  \n","Max_valid_Acc = 0 \n","Max_test_Acc = 0                  \n","Max_test_Recall = 0\n","Max_BLC_Test_Recall = 0\n","Max_test_overall_metric = 0\n","i = -1\n","pbar = tqdm(params)\n","for p in pbar:\n","    i += 1\n","    model_name =  export_name + 'Model' + str(i) + '_' + date_hour\n","    torch.cuda.empty_cache()\n","    Model_ = Model_Training_with_loader(**p,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset, testDataset=testDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=p)\n","    np.random.seed(0)\n","    random.seed(0)\n","    torch.manual_seed(0)\n","    Model_.train()\n","\n","    train_accuracy = max(Model_.Epochs_Train_Acc)\n","    # valid_accuracy = max(Model_.Epochs_Val_Acc)\n","    # test_accuracy = max(Model_.Epochs_test_Acc)\n","    train_epoch_loss = Model_.Epochs_Train_loss\n","    train_epoch_acc = Model_.Epochs_Train_Acc\n","    valid_epoch_loss = Model_.Epochs_Val_loss\n","    valid_epoch_acc = Model_.Epochs_Val_Acc\n","    test_epoch_loss = Model_.Epochs_test_loss\n","    test_epoch_acc = Model_.Epochs_test_Acc\n","\n","\n","    model_ = Model_.model.eval().to('cpu')\n","    # test Avg. Sensitivity\n","    X = input_data[2]\n","    Y = labels[2]\n","    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","    test_accuracy = sk.metrics.accuracy_score(Y, Y_pred )\n","    test_precision, test_recall, test_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n","    # Valid Avg. Sensitivity\n","    X = input_data[1]\n","    Y = labels[1]\n","    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","    valid_accuracy = sk.metrics.accuracy_score(Y, Y_pred )\n","    valid_precision, valid_recall, valid_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n","    # Balanced test set Avg. Sensitivity\n","    X = Evaluation_set[0]\n","    Y = Evaluation_set[1]\n","    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","    blc_test_precision, blc_test_recall, blc_test_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n","\n","    test_overall_metric = (test_accuracy + test_recall + blc_test_recall ) / 3\n","    if test_overall_metric > Max_test_overall_metric: pickle.dump(model_ , open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  'Best_Grid_Model' +str(i) +'_' + date_hour, 'wb'))\n","\n","    if train_accuracy > Max_train_Acc: Max_train_Acc = train_accuracy\n","    if valid_accuracy > Max_valid_Acc: Max_valid_Acc = valid_accuracy\n","    if test_accuracy > Max_test_Acc: Max_test_Acc = test_accuracy\n","    if test_recall > Max_test_Recall: Max_test_Recall = test_recall\n","    if blc_test_recall > Max_BLC_Test_Recall: Max_BLC_Test_Recall = blc_test_recall\n","\n","    pbar.set_description(f'Max_Test_Recall: {Max_test_Recall:.1%}, Max_BLC_Test_Recall: {Max_BLC_Test_Recall:.1%}, Max_Tr_Acc: {Max_train_Acc:.1%}, Max_V_Acc: {Max_valid_Acc:.1%}, Max_Te_Acc: {Max_test_Acc:.1%} ')\n","    pbar.set_postfix_str(f'Train_Acc: {train_accuracy:.1%} , Valide_Acc: {valid_accuracy:.1%} , Test_Acc: {test_accuracy:.1%}')\n","\n","    new_row = pd.Series({'model_name':model_name,'hyper_param':p,'test_overall_metric':test_overall_metric,'test_recall':test_recall,'blc_test_recall':blc_test_recall,'valid_recall':valid_recall,'train_accuracy':train_accuracy,'valid_accuracy':valid_accuracy,'test_accuracy':test_accuracy,\n","                         'train_epoch_loss':train_epoch_loss,'train_epoch_acc':train_epoch_acc,'valid_epoch_loss':valid_epoch_loss,'valid_epoch_acc':valid_epoch_acc,'test_epoch_acc':test_epoch_acc,'test_epoch_loss':test_epoch_loss}, name='')\n","    Hyper_Details = Hyper_Details.append(new_row)\n","    m_details = pd.DataFrame()\n","    m_details = m_details.append(new_row)\n","\n","\n","\n","\n","    if save_results:\n","      # Hyper_Details.to_csv(G_path +'/Saved/Grid_Search_Results/' + export_name + date_hour+ '.csv')\n","      \n","      pickle.dump(m_details, open(G_path +'/08_Saved_Models_Outpus/Grid_Search_Results/' + model_name, 'wb'))\n","      pickle.dump(Hyper_Details, open(G_path +'/08_Saved_Models_Outpus/Grid_Search_Results/' + 'Grid_All_' + export_name + date_hour , 'wb'))\n","\n","    if save_models:\n","      pickle.dump(model_, open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  model_name, 'wb'))\n","\n","    Best_Grid_Model = pickle.load(open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  'Best_Grid_Model' +str(i) +'_' + date_hour, 'rb'))\n","\n","Hyper_Details.sort_values('test_overall_metric', ascending=False,inplace=True)\n","best_param = Hyper_Details['hyper_param'][0]\n","best_params = Hyper_Details['hyper_param'][:4]\n","\n","# Grid_Details = Hyper_Details\n","\n","# Model_Grid = Model_Training_with_loader(**best_param,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=best_param)\n","# np.random.seed(0)\n","# random.seed(0)\n","# torch.manual_seed(0)\n","# Model_Grid.train()\n","# print('Max Avg. Recall on Test ====>> :')\n","# Grid_Details['test_recall'][:5], Grid_Details['hyper_param'][0]"]},{"cell_type":"code","source":["Model_Training_with_loader()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"fgImOE9cYnJi","executionInfo":{"status":"error","timestamp":1657495258904,"user_tz":-120,"elapsed":277,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}},"outputId":"91f1dc56-dba7-4a8f-a226-b196b75b19f3"},"execution_count":30,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-d02e52c4a76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_Training_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: __init__() missing 18 required positional arguments: 'Net', 'Drop', 'LR', 'batch_size', 'Momentum', 'epochs', 'patience', 'weight_decay', 'loss_func', 'opt_func', 'w_sampler', 'trainDataset', 'validDataset', 'X_test', 'Y_test', 'print_epochs', 'hyper_params', and 'device'"]}]},{"cell_type":"code","source":["print_epochs = True\n","save_results = False\n","save_models = False\n","cross_valid = False\n","nr_repeat =1\n","export_name = 'CNN_280x210_MultiClass' # {'280_210_','AutoEncoder_'}\n","kflods = sk.model_selection.KFold(n_splits=10, shuffle=True)\n","\n","NNs_grid_searc_cross_valid_trainer(params, X_train ,Y_train, X_valid, Y_valid, X_test, Y_test, kflods, print_epochs, save_results, save_models, cross_valid, nr_repeat, export_name)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"-SNnwKy1XMVr","executionInfo":{"status":"error","timestamp":1657551603808,"user_tz":-120,"elapsed":15549,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}},"outputId":"2d129bef-08b2-45db-f9a5-7c52b43a5902"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":[""]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-eff8a73ef35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mkflods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mNNs_grid_searc_cross_valid_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkflods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_repeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-72d64d34875f>\u001b[0m in \u001b[0;36mNNs_grid_searc_cross_valid_trainer\u001b[0;34m(params, X_train, Y_train, X_valid, Y_valid, X_test, Y_test, kflods, print_epochs, save_results, save_models, cross_valid, nr_repeat, export_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mexport_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Model'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdate_hour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mModel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_Training_with_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_sampler\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrainDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestDataset\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_epochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mprint_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DataSources/SkinCare/features/usefull_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, Net, Drop, LR, batch_size, Momentum, epochs, patience, weight_decay, loss_func, opt_func, w_sampler, trainDataset, validDataset, testDataset, X_test, Y_test, print_epochs, hyper_params, device)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mopt_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["def NNs_grid_searc_cross_valid_trainer(params, X_train ,Y_train, X_valid, Y_valid, X_test, Y_test, kflods, print_epochs, save_results, save_models, cross_valid, nr_repeat, export_name):\n","\n","    Param_Details = pd.DataFrame(columns=['model_name','hyper_param','test_accuracy','valid_accuracy','train_accuracy','test_specificity','valid_specificity'\n","                                      ,'train_epoch_loss','train_epoch_acc','valid_epoch_loss','valid_epoch_acc','test_epoch_acc','test_epoch_loss','test_fpr','test_tpr','test_roc_auc','train_index','valid_index'])\n","    date_hour = datetime.now().strftime(\"%d_%b%y_%H-%M\") \n","    Max_train_Acc = 0  \n","    Max_valid_Acc = 0 \n","    Max_test_Acc = 0                  \n","    Max_test_Recall = 0\n","    Max_BLC_Test_Recall = 0\n","    Max_test_overall_metric = 0\n","\n","    if cross_valid:\n","        total_iter = nr_repeat*len(params)*kflods.get_n_splits()\n","    else: total_iter = nr_repeat*len(params)\n","    iter = 1\n","\n","    for i in range(nr_repeat):\n","        for p in params:\n","            test_accuracy = []\n","            test_fscore = []\n","            test_precision = []\n","            test_recall = []\n","            train_index = None\n","            valid_index = None\n","            if cross_valid:\n","                training_data = torch.cat([X_train,X_valid],dim=0)\n","                training_label = torch.cat([Y_train,Y_valid],dim=0)\n","                \n","                for fold, (train_index, valid_index) in enumerate(kflods.split(training_data,training_label)):\n","                    progressbar(iter,total_iter, \"Interations: \", 100)\n","                    iter +=1\n","                    x_train_ = training_data[train_index]\n","                    y_train_ = training_label[train_index]\n","                    x_valid_ = training_data[valid_index]\n","                    y_valid_ = training_label[valid_index]\n","                    \n","                    model_name =  export_name + 'Model' + str(iter) + '_' + date_hour\n","                    torch.cuda.empty_cache()\n","                    Model_ = Model_Training_with_loader(**p,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset, testDataset=testDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=p, device=device)\n","                    np.random.seed(0)\n","                    random.seed(0)\n","                    torch.manual_seed(0)\n","                    torch.cuda.manual_seed(0)\n","                    torch.backends.cudnn.deterministic = True\n","                    Model_.train()\n","\n","                    train_accuracy = max(Model_.Epochs_Train_Acc)\n","                    train_epoch_loss = Model_.Epochs_Train_loss\n","                    train_epoch_acc = Model_.Epochs_Train_Acc\n","                    valid_epoch_loss = Model_.Epochs_Val_loss\n","                    valid_epoch_acc = Model_.Epochs_Val_Acc\n","                    test_epoch_loss = Model_.Epochs_test_loss\n","                    test_epoch_acc = Model_.Epochs_test_Acc\n","\n","                    model_ = Model_.model.eval().to('cpu')\n","                    # test Performance\n","                    X = X_test\n","                    Y = Y_test\n","                    Y_OneH = sk.preprocessing.label_binarize(Y,classes=np.unique(Y))\n","                    Y_pred_prob = model_.forward_noDrop(X)\n","                    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","                    per_ = recall_specificity(Y,Y_pred).loc['Weighted Avg.']\n","                    test_accuracy , test_specificity = per_['Recall_Sensitivity'] , per_['Specificity']\n","                    test_fpr, test_tpr, test_roc_auc = fpr_tpr_score(Y_OneH,Y_pred_prob)\n","\n","                    # Valid performance\n","                    X = X_valid\n","                    Y = Y_valid\n","                    Y_OneH = sk.preprocessing.label_binarize(Y,classes=np.unique(Y))\n","                    Y_pred_prob = model_.forward_noDrop(X)\n","                    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","                    per_ = recall_specificity(Y,Y_pred).loc['Weighted Avg.']\n","                    valid_accuracy , valid_specificity = per_['Recall_Sensitivity'] , per_['Specificity']\n","\n","                    new_row = pd.Series({'model_name':model_name,'hyper_param':p,'test_accuracy':test_accuracy,'valid_accuracy':valid_accuracy,'train_accuracy':train_accuracy,'test_specificity':test_specificity,'valid_specificity':valid_specificity\n","                                      ,'train_epoch_loss':train_epoch_loss,'train_epoch_acc':train_epoch_acc,'valid_epoch_loss':valid_epoch_loss,'valid_epoch_acc':valid_epoch_acc,'test_epoch_acc':test_epoch_acc,'test_epoch_loss':test_epoch_loss\n","                                      ,'test_fpr':test_fpr,'test_tpr':test_tpr,'test_roc_auc':test_roc_auc,'train_index':train_index,'valid_index':valid_index}, name='')\n","                    Param_Details = Param_Details.append(new_row)\n","            else:\n","                    progressbar(iter,total_iter, \"Interations: \", 100)\n","                    iter +=1\n","                    x_train_ = X_train\n","                    y_train_ = Y_train\n","                    x_valid_ = X_valid\n","                    y_valid_ = Y_valid\n","                    \n","                    model_name =  export_name + 'Model' + str(iter) + '_' + date_hour\n","                    torch.cuda.empty_cache()\n","                    Model_ = Model_Training_with_loader(**p,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset, testDataset=testDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=p, device=device)\n","                    np.random.seed(0)\n","                    random.seed(0)\n","                    torch.manual_seed(0)\n","                    torch.cuda.manual_seed(0)\n","                    torch.backends.cudnn.deterministic = True\n","                    Model_.train()\n","\n","                    train_accuracy = max(Model_.Epochs_Train_Acc)\n","                    train_epoch_loss = Model_.Epochs_Train_loss\n","                    train_epoch_acc = Model_.Epochs_Train_Acc\n","                    valid_epoch_loss = Model_.Epochs_Val_loss\n","                    valid_epoch_acc = Model_.Epochs_Val_Acc\n","                    test_epoch_loss = Model_.Epochs_test_loss\n","                    test_epoch_acc = Model_.Epochs_test_Acc\n","\n","                    model_ = Model_.model.eval().to('cpu')\n","                    # test Performance\n","                    X = X_test\n","                    Y = Y_test\n","                    Y_OneH = sk.preprocessing.label_binarize(Y,classes=np.unique(Y))\n","                    Y_pred_prob = model_.forward_noDrop(X)\n","                    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","                    per_ = recall_specificity(Y,Y_pred).loc['Weighted Avg.']\n","                    test_accuracy , test_specificity = per_['Recall_Sensitivity'] , per_['Specificity']\n","                    test_fpr, test_tpr, test_roc_auc = fpr_tpr_score(Y_OneH,Y_pred_prob)\n","\n","                    # Valid performance\n","                    X = X_valid\n","                    Y = Y_valid\n","                    Y_OneH = sk.preprocessing.label_binarize(Y,classes=np.unique(Y))\n","                    Y_pred_prob = model_.forward_noDrop(X)\n","                    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","                    per_ = recall_specificity(Y,Y_pred).loc['Weighted Avg.']\n","                    valid_accuracy , valid_specificity = per_['Recall_Sensitivity'] , per_['Specificity']\n","\n","                    new_row = pd.Series({'model_name':model_name,'hyper_param':p,'test_accuracy':test_accuracy,'valid_accuracy':valid_accuracy,'train_accuracy':train_accuracy,'test_specificity':test_specificity,'valid_specificity':valid_specificity\n","                                      ,'train_epoch_loss':train_epoch_loss,'train_epoch_acc':train_epoch_acc,'valid_epoch_loss':valid_epoch_loss,'valid_epoch_acc':valid_epoch_acc\n","                                      ,'test_epoch_acc':test_epoch_acc,'test_epoch_loss':test_epoch_loss,'test_fpr':test_fpr,'test_tpr':test_tpr,'test_roc_auc':test_roc_auc,'train_index':train_index,'valid_index':valid_index}, name='')\n","                    Param_Details = Param_Details.append(new_row)\n","\n","\n","    # best_one = np.argmax(Param_Details.test_recall_weighed)\n","    # best_param = Param_Details.iloc[best_one]['hyper_param']\n","    # best_train_index = Param_Details.iloc[best_one]['train_index'] \n","    # best_valid_index = Param_Details.iloc[best_one]['valid_index']  \n","    # if cross_valid:\n","    #         x_train_ = training_data[best_train_index]\n","    #         y_train_ = training_label[best_train_index]\n","    #         x_valid_ = training_data[best_valid_index]\n","    #         y_valid_ = training_label[best_valid_index]\n","\n","    # Best_Model = Model_.set_params(**best_param)\n","    # Best_Model.fit(x_train_, y_train_)\n","    # Y_pred = Best_Model.predict(X_test)\n","    # Param_Details['hyper_param'] = Param_Details['hyper_param'].apply(lambda x: json.dumps(x))\n","    # print('------- Precision recal %--------')\n","    # print(sk.metrics.classification_report(Y_test,Y_pred))\n","    # print('Best param: ' , best_param)\n","    # return Best_Model, Param_Details"],"metadata":{"id":"1a-CkgRSO1KX","executionInfo":{"status":"ok","timestamp":1657551585813,"user_tz":-120,"elapsed":352,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVnEzpk9FGTb"},"outputs":[],"source":["# model_ = Model_Grid\n","model_1 = Best_Grid_Model\n","X = input_data[2]\n","Y = labels[2]\n","# X = Evaluation_set[0]\n","# Y = Evaluation_set[1]\n","Y_pred = model_1.forward_noDrop(X).argmax(dim=1)\n","results = confusion_matrix(Y,Y_pred)\n","# plot_loss_accuracy(model_)\n","results"]},{"cell_type":"markdown","metadata":{"id":"zomymNkU4JNX"},"source":["# **Cross Validation: Repeat 5 times with 10 Kfold (overall 50 models)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qWnKjVHfeBE"},"outputs":[],"source":["from torch.nn.modules import loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLfqTs-54anr"},"outputs":[],"source":["# K_Folds_test = sk.model_selection.KFold(n_splits=10, shuffle=True)\n","K_Folds_valid = sk.model_selection.KFold(n_splits=10, shuffle=True)\n","dat_hour = datetime.now().strftime(\"%d_%b_%Y_%H_%M\")\n","print_epochs = False\n","\n","param1 = {\n","    'Net':CNN_Nets.Net8_b\n","    ,'Drop':0.2\n","    ,'LR':1.1e-3\n","    ,'batch_size':24\n","    ,'Momentum':0.77\n","    ,'epochs':100\n","    ,'patience':20\n","    ,'weight_decay':1e-3\n","    ,'loss_func' : \n","    loss.CrossEntropyLoss\n","    ,'opt_func' : \n","    torch.optim.SGD\n","}\n","\n","CV_data = torch.cat([input_data[0],input_data[1]],dim=0)\n","CV_label = torch.cat([labels[0],labels[1]],dim=0)\n","X_test = input_data[2]\n","Y_test = labels[2]\n","\n","\n","CV_Details = pd.DataFrame(columns=['hyper_param','train_recall_weighed','valid_recall_weighed','test_recall_weighed'\n","                                  ,'valid_recall_simple','test_recall_simple'\n","                                  ,'valid_specificity_weighed','test_specificity_weighed'\n","                                  ,'valid_specificity_simple','test_specificity_simple'])\n","\n","Max_train_Acc = 0  \n","Max_valid_Acc = 0 \n","Max_test_Acc = 0                  \n","\n","pbar1 = tqdm(range(5))\n","for i in pbar1:\n","    pbar2 = tqdm(enumerate(K_Folds_valid.split(CV_data)), total=K_Folds_valid.get_n_splits())\n","    for fold, (training_index, valid_index) in pbar2:\n","        X_train = CV_data[training_index]\n","        Y_train = CV_label[training_index]\n","        X_valid = CV_data[valid_index]\n","        Y_valid = CV_label[valid_index]\n","\n","        label_freq = np.bincount(Y_train)\n","        labels_weights = 1. / label_freq\n","        weights = labels_weights[Y_train]\n","        w_sampler = WeightedRandomSampler(weights, len(weights))\n","\n","        trainDataset = dset(X_train, Y_train)\n","        validDataset = dset(X_valid, Y_valid)\n","        testDataset = dset(X_test, Y_test)\n","\n","        Model_ = Model_Training_with_loader(**param1,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=param1)\n","        Model_.train()\n","\n","        model_ = Model_.model.eval().to('cpu')\n","            \n","        # Train\n","        train_recall_weighed = mean(Model_.Epochs_Train_Acc)\n","\n","        # Valid\n","        Y_pred = model_(X_valid).argmax(axis=1)\n","        Y = Y_valid\n","        valid_recall_weighed, valid_specificity_weighed, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=True)\n","        valid_recall_simple, valid_specificity_simple, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=False)\n","\n","        # Test\n","        Y_pred = model_(X_test).argmax(axis=1)\n","        Y = Y_test\n","        test_recall_weighed, test_specificity_weighed, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=True)\n","        test_recall_simple, test_specificity_simple, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=False)\n","\n","\n","        if train_recall_weighed > Max_train_Acc: Max_train_Acc = train_recall_weighed\n","        if valid_recall_weighed > Max_valid_Acc: Max_valid_Acc = valid_recall_weighed\n","        if test_recall_weighed > Max_test_Acc: Max_test_Acc = test_recall_weighed\n","        new_row = pd.Series({'train_recall_weighed':train_recall_weighed,'valid_recall_weighed':valid_recall_weighed,'test_recall_weighed':test_recall_weighed\n","                                      ,'valid_recall_simple':valid_recall_simple,'test_recall_simple':test_recall_simple\n","                                      ,'valid_specificity_weighed':valid_specificity_weighed,'test_specificity_weighed':test_specificity_weighed\n","                                      ,'valid_specificity_simple':valid_specificity_simple,'test_specificity_simple':test_specificity_simple}, name='')\n","        CV_Details = CV_Details.append(new_row)\n","        pbar1.set_description(f'Max_train_Acc: {Max_train_Acc:.1%}, Max_valid_Acc: {Max_valid_Acc:.1%}, Max_test_Acc: {Max_test_Acc:.1%} ')\n","        pbar1.set_postfix_str(f'Train_Acc: {train_recall_weighed:.1%} , Valide_Acc: {valid_recall_weighed:.1%} , Test_Acc: {test_recall_weighed:.1%}')\n","\n","\n","        pickle.dump(CV_Details, open(G_path + '/08_Saved_Models_Outpus/Cross_Valid_Results/CV_280_MultiClass_'+ dat_hour,'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zKshOw9PaDt"},"outputs":[],"source":["col_recall = ['test_recall_weighed','valid_recall_weighed']\n","model_recalls = CV_Details[col_recall]\n","col_specificity = ['test_specificity_weighed','valid_specificity_weighed']\n","model_specificity = CV_Details[col_specificity]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYPZ9JOfPYIe"},"outputs":[],"source":["plt.subplots(figsize=(8,4))\n","sns.boxplot(data=model_recalls)\n","plt.title('Test performance of 100 Cross-Validation',fontsize = 18)\n","plt.xlabel(\"Classifiers\", fontsize = 14)\n","plt.ylabel(\"Accuracy\", fontsize = 14)\n","plt.xticks(fontsize=16, rotation=0)\n","plt.show()\n","\n","plt.subplots(figsize=(8,4))\n","sns.boxplot(data=model_specificity)\n","plt.title('Valid performance of 100 Cross-Validation',fontsize = 18)\n","plt.xlabel(\"Classifiers\", fontsize = 14)\n","plt.ylabel(\"Accuracy\", fontsize = 14)\n","plt.xticks(fontsize=16, rotation=0)\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"06_CNN_280x210_MultiClass.ipynb","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}