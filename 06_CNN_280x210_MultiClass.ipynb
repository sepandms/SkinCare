{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22176,"status":"ok","timestamp":1657439584000,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"},"user_tz":-120},"id":"MzRoZIkXDiT7","outputId":"dcf15fd5-6d72-4f8a-aaaa-a44b640e390d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  import os\n","  CWD = '/content/drive/MyDrive/DataSources/SkinCare'\n","  os.chdir(CWD)\n","except:None\n","G_path = './Project_Data'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7935,"status":"ok","timestamp":1657439591929,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"},"user_tz":-120},"id":"VZrZXitoAlDd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a03de659-e801-4801-eb91-569513ae9819"},"outputs":[{"output_type":"stream","name":"stdout","text":["devis: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset as dset\n","#import torchvision.transforms.Compose\n","import numpy as np\n","from datetime import datetime\t\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.utils.data import random_split\n","from torch.utils.data import SubsetRandomSampler\n","from torch.utils.data import WeightedRandomSampler\n","from torch.utils.data import DataLoader\n","import time\n","from tqdm import tqdm\n","import pickle as pickle\n","from statistics import mean\n","import pandas as pd\n","pd.options.display.max_colwidth = 250\n","import sklearn as sk\n","from sklearn.model_selection import train_test_split\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else \"cpu\")\n","from sklearn.utils import resample\n","import warnings\n","warnings.filterwarnings('ignore')\n","import features\n","from features.usefull_functions import *\n","from features.NETs import *\n","from features.Model_Training import *\n","device = None\n","try:\n","    c = torch.cuda.is_available()\n","    if c:\n","        print('devis: cuda')\n","        device = 'cuda'\n","    else:\n","        try :\n","            m = torch.backends.mps.is_available()\n","            if m:\n","                device = 'mps'\n","                print('devis: mps')  \n","        except:    \n","            device = 'cpu'\n","            print('devis: cpu')           \n","except:\n","    None"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":59778,"status":"ok","timestamp":1657439651700,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"},"user_tz":-120},"id":"6y2NEQ6WOvNi"},"outputs":[],"source":["Binary_classification = False\n","file = 'Input_DataSet_280x210' \n","input_data , labels = pickle.load(open(G_path + '/06_Rescaled_DataSet/'+ file,'rb'))\n","file = 'Balanced_Test_Set_280x210' \n","Evaluation_set = pickle.load(open(G_path + '/06_Rescaled_DataSet/' + file,'rb'))\n","\n","if Binary_classification:\n","  labels = labels[1]\n","else:\n","  labels = labels[0]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_Mz7iN2hFgF","outputId":"b4fbec3f-e21b-4c95-de95-0c98c1134652","executionInfo":{"status":"ok","timestamp":1657440299730,"user_tz":-120,"elapsed":417,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5, 5, 4, 5, 2, 1, 5, 2, 5, 5])"]},"metadata":{},"execution_count":4}],"source":["labels[1][:10]"]},{"cell_type":"markdown","metadata":{"id":"YX40UaIYFt0J"},"source":["# **CNN Networks: Configuration**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HghWL1MnzLFX","executionInfo":{"status":"ok","timestamp":1657440307210,"user_tz":-120,"elapsed":4962,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}},"outputId":"59893bf9-d7c3-4cd7-d8c7-9bf58faa25c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Net1\n","202752\n","Net10\n","4480\n","Net11\n","1080\n","Net2\n","221952\n","Net3\n","12288\n","Net4\n","12288\n","Net5\n","56304\n","Net6\n","55488\n","Net7\n","384\n","Net8\n","384\n","Net8_a\n","384\n","Net8_a_binary\n","1536\n","Net8_b\n","1536\n","Net8_b_binary\n","1536\n","Net9\n","384\n"]}],"source":["net_list = [func for func in dir(CNN_Nets) if callable(getattr(CNN_Nets, func)) and not func.startswith(\"__\")]\n","X_ = input_data[0][0:5]\n","for net in net_list:\n","    print(net)\n","    model_ = getattr(CNN_Nets,net)\n","    fc_features = model_().dimention_set(X_)\n","    print(fc_features)\n","    model_ = getattr(CNN_Nets,net)\n","    model_.fc_features = fc_features\n","    out = model_().forward(X_)\n","    setattr(CNN_Nets,net,model_)\n","    # print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"XWC5a989AlDg"},"source":["# **Metrics and Performance**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pBE96mxDAlDg","executionInfo":{"status":"ok","timestamp":1657440314691,"user_tz":-120,"elapsed":481,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[],"source":["def recall_specificity(Y,Y_pred, type):\n","    CM = sk.metrics.confusion_matrix(Y,Y_pred)\n","    FP = CM.sum(axis=0) - np.diag(CM) \n","    FN = CM.sum(axis=1) - np.diag(CM)\n","    TP = np.diag(CM)\n","    TN = CM.sum() - (FP + FN + TP)\n","    weights = CM.sum(axis=1) / CM.sum() \n","    ACC = np.nan_to_num((TP+TN)/(TP+FP+FN+TN) , nan=0)\n","    Recall_Sensitivity = np.nan_to_num(TP/(TP+FN) , nan=0)\n","    Specificity = np.nan_to_num(TN/(TN+FP) , nan=0)\n","    Precision = np.nan_to_num(TP/(TP+FP) , nan=0)\n","    f1_score = np.nan_to_num( 2*Precision*Recall_Sensitivity / (Recall_Sensitivity + Precision), nan=0)\n","    Performance_DF = pd.concat([pd.DataFrame(CM),pd.DataFrame(weights, columns=['weights']),pd.DataFrame(Precision, columns=['Precision']),pd.DataFrame(Recall_Sensitivity,columns=['Recall_Sensitivity'])\n","        ,pd.DataFrame(Specificity, columns=['Specificity']),pd.DataFrame(f1_score, columns=['f1_score'])], axis=1)\n","    total_row1 = pd.Series({'Precision':mean(Precision),'Recall_Sensitivity':mean(Recall_Sensitivity),'Specificity':mean(Specificity),'f1_score':mean(f1_score)}, name='Simple Avg.')\n","    total_row2 = pd.Series({'Precision':sum(weights*Precision),'Recall_Sensitivity':sum(weights*Recall_Sensitivity),'Specificity':sum(weights*Specificity),'f1_score':sum(weights*f1_score)}, name='Weighted Avg.')\n","    Performance_DF = Performance_DF.append([total_row1,total_row2])\n","    cols = ['weights','Precision','Recall_Sensitivity','Specificity','f1_score']\n","    return per_details\n","\n","def plot_loss_accuracy(model_):\n","    epochs_X = [i for i in range(1, len(model_.Epochs_Train_loss)+1)]\n","    fig, axs = plt.subplots(1,2,figsize=(14,4))\n","    axs[0].plot(epochs_X , model_.Epochs_Train_loss , 'bo-', label='Train loss')\n","    axs[0].plot(epochs_X , model_.Epochs_Val_loss,'ro-', label='Validation loss')\n","    axs[0].plot(epochs_X , model_.Epochs_test_loss,'go-', label='Test loss')\n","    axs[0].set_xlabel(\"Epochs\", fontsize = 12)\n","    axs[0].set_ylabel(\"Loss\", fontsize = 12)\n","    axs[0].grid()\n","    axs[0].legend()\n","    axs[0].set_title('Train and Validation loss by epochs', fontsize = 14)\n","    axs[1].plot(epochs_X , model_.Epochs_Train_Acc , 'bo-', label='Train Accuracy')\n","    axs[1].plot(epochs_X , model_.Epochs_Val_Acc ,'ro-', label='Validation Accuracy')\n","    axs[1].plot(epochs_X , model_.Epochs_test_Acc ,'go-', label='Test Accuracy')\n","    axs[1].set_xlabel(\"Epochs\", fontsize = 12)\n","    axs[1].set_ylabel(\"Accuracy\", fontsize = 12)\n","    axs[1].grid()\n","    axs[1].legend()\n","    axs[1].set_title('Train and Validation Accuracy by epochs', fontsize = 14)\n","    plt.show()\n","\n","def recall_specificity_precision(Y,Y_pred, weighted_avg):\n","    CM = sk.metrics.confusion_matrix(Y,Y_pred)\n","    FP = CM.sum(axis=0) - np.diag(CM) \n","    FN = CM.sum(axis=1) - np.diag(CM)\n","    TP = np.diag(CM)\n","    TN = CM.sum() - (FP + FN + TP)\n","    weights = CM.sum(axis=1) / CM.sum() \n","    Accuracy = np.nan_to_num((TP+TN)/(TP+FP+FN+TN) , nan=0)\n","    Recall = np.nan_to_num(TP/(TP+FN) , nan=0)\n","    Specificity = np.nan_to_num(TN/(TN+FP) , nan=0)\n","    Precision = np.nan_to_num(TP/(TP+FP) , nan=0)\n","    if weighted_avg:\n","        return round(sum(weights*Recall),3), round(sum(weights*Specificity),3), round(sum(weights*Precision),3)\n","    else:\n","        return round(mean(Recall),3), round(mean(Specificity),3), round(mean(Precision),3)"]},{"cell_type":"markdown","metadata":{"id":"0w-7sG3Slop1"},"source":["# **Define Training data**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"sM2zFYZBWR45","executionInfo":{"status":"ok","timestamp":1657440317731,"user_tz":-120,"elapsed":1,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[],"source":["X_train = input_data[0]\n","Y_train = labels[0]\n","X_valid = input_data[1]\n","Y_valid = labels[1]\n","X_test = input_data[2]\n","Y_test = labels[2]\n","\n","\n","label_freq = np.bincount(Y_train)\n","labels_weights = 1. / label_freq\n","weights = labels_weights[Y_train]\n","w_sampler = WeightedRandomSampler(weights, len(weights))\n","\n","trainDataset = dset(X_train, Y_train)\n","validDataset = dset(X_valid, Y_valid)\n","testDataset = dset(X_test, Y_test)"]},{"cell_type":"markdown","metadata":{"id":"geKyTRJ8AlDh"},"source":["# **Model by Grid**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VL5FxPLAAlDi","executionInfo":{"status":"ok","timestamp":1657440323188,"user_tz":-120,"elapsed":418,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[],"source":["\n","Net = [CNN_Nets.Net8_b,CNN_Nets.Net8_a]\n","Drop = [0.25]\n","LR = [ 1.1e-3]\n","batch_size = [24]\n","Momentum = [0.77]\n","epochs = [2]\n","patience = [20]\n","weight_decay = [1e-3]\n","loss_func  =  [nn.CrossEntropyLoss]\n","opt_func = [torch.optim.SGD]\n","\n","\n","grid = {\n","    'Net' : Net\n","    ,'Drop' : Drop\n","    ,'LR' : LR\n","    ,'batch_size' : batch_size\n","    ,'Momentum' : Momentum\n","    ,'epochs' : epochs\n","    ,'patience': patience\n","    ,'weight_decay' :weight_decay\n","    ,'loss_func'  :loss_func\n","    ,'opt_func' : opt_func\n","}\n","params = sk.model_selection.ParameterGrid(grid)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEkHc_7xAlDi","outputId":"155e206a-47fb-47ae-aba8-f3fabb1dbde7","executionInfo":{"status":"ok","timestamp":1657384615521,"user_tz":-120,"elapsed":42599,"user":{"displayName":"sepehr sp","userId":"09755144277486379277"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["[Epoch: 1]  , Train_loss: 1.9 , Train_Acc: 17.3%, Val_loss: 1.8 , Val_Acc: 41.7%, Test_Acc: 36.8%  , run time: 6.1\n","[Epoch: 2]  , Train_loss: 1.8 , Train_Acc: 26.4%, Val_loss: 1.7 , Val_Acc: 40.0%, Test_Acc: 35.0%  , run time: 6.02\n"]},{"output_type":"stream","name":"stderr","text":["Max_Test_Recall: 27.3%, Max_BLC_Test_Recall: 25.7%, Max_Tr_Acc: 26.4%, Max_V_Acc: 40.0%, Max_Te_Acc: 35.1% :  50%|█████     | 1/2 [00:20<00:20, 20.31s/it, Train_Acc: 26.4% , Valide_Acc: 40.0% , Test_Acc: 35.1%]"]},{"output_type":"stream","name":"stdout","text":["[Epoch: 1]  , Train_loss: 1.9 , Train_Acc: 16.6%, Val_loss: 1.8 , Val_Acc: 43.6%, Test_Acc: 39.0%  , run time: 6.59\n","[Epoch: 2]  , Train_loss: 1.8 , Train_Acc: 24.6%, Val_loss: 1.7 , Val_Acc: 40.7%, Test_Acc: 36.5%  , run time: 6.62\n"]},{"output_type":"stream","name":"stderr","text":["Max_Test_Recall: 27.5%, Max_BLC_Test_Recall: 27.6%, Max_Tr_Acc: 26.4%, Max_V_Acc: 40.8%, Max_Te_Acc: 36.6% : 100%|██████████| 2/2 [00:42<00:00, 21.07s/it, Train_Acc: 24.6% , Valide_Acc: 40.8% , Test_Acc: 36.6%]\n"]}],"source":["print_epochs = True\n","save_results = False\n","save_models = False\n","cross_valid = False\n","nr_repeat =1\n","\n","export_name = 'CNN_280x210_MultiClass' # {'280_210_','AutoEncoder_'}\n","\n","Hyper_Details = pd.DataFrame(columns=['model_name','hyper_param','test_accuracy','valid_accuracy','train_accuracy','test_specificity','valid_specificity','train_epoch_loss','train_epoch_acc','valid_epoch_loss','valid_epoch_acc','test_epoch_acc','test_epoch_loss',''])\n","date_hour = datetime.now().strftime(\"%d_%b%y_%H-%M\") \n","Max_train_Acc = 0  \n","Max_valid_Acc = 0 \n","Max_test_Acc = 0                  \n","Max_test_Recall = 0\n","Max_BLC_Test_Recall = 0\n","Max_test_overall_metric = 0\n","i = -1\n","pbar = tqdm(params)\n","for p in pbar:\n","    i += 1\n","    model_name =  export_name + 'Model' + str(i) + '_' + date_hour\n","    torch.cuda.empty_cache()\n","    Model_ = Model_Training_with_loader(**p,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=p)\n","    np.random.seed(0)\n","    random.seed(0)\n","    torch.manual_seed(0)\n","    Model_.train()\n","\n","    train_accuracy = max(Model_.Epochs_Train_Acc)\n","    # valid_accuracy = max(Model_.Epochs_Val_Acc)\n","    # test_accuracy = max(Model_.Epochs_test_Acc)\n","    train_epoch_loss = Model_.Epochs_Train_loss\n","    train_epoch_acc = Model_.Epochs_Train_Acc\n","    valid_epoch_loss = Model_.Epochs_Val_loss\n","    valid_epoch_acc = Model_.Epochs_Val_Acc\n","    test_epoch_loss = Model_.Epochs_test_loss\n","    test_epoch_acc = Model_.Epochs_test_Acc\n","\n","\n","    model_ = Model_.model.eval().to('cpu')\n","    # test Avg. Sensitivity\n","    X = input_data[2]\n","    Y = labels[2]\n","    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","    test_accuracy = sk.metrics.accuracy_score(Y, Y_pred )\n","    test_precision, test_recall, test_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n","    # Valid Avg. Sensitivity\n","    X = input_data[1]\n","    Y = labels[1]\n","    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","    valid_accuracy = sk.metrics.accuracy_score(Y, Y_pred )\n","    valid_precision, valid_recall, valid_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n","    # Balanced test set Avg. Sensitivity\n","    X = Evaluation_set[0]\n","    Y = Evaluation_set[1]\n","    Y_pred = model_.forward_noDrop(X).argmax(dim=1)\n","    blc_test_precision, blc_test_recall, blc_test_fscore, m = sk.metrics.precision_recall_fscore_support(Y, Y_pred, average = 'macro')\n","\n","    test_overall_metric = (test_accuracy + test_recall + blc_test_recall ) / 3\n","    if test_overall_metric > Max_test_overall_metric: pickle.dump(model_ , open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  'Best_Grid_Model' +str(i) +'_' + date_hour, 'wb'))\n","\n","    if train_accuracy > Max_train_Acc: Max_train_Acc = train_accuracy\n","    if valid_accuracy > Max_valid_Acc: Max_valid_Acc = valid_accuracy\n","    if test_accuracy > Max_test_Acc: Max_test_Acc = test_accuracy\n","    if test_recall > Max_test_Recall: Max_test_Recall = test_recall\n","    if blc_test_recall > Max_BLC_Test_Recall: Max_BLC_Test_Recall = blc_test_recall\n","\n","    pbar.set_description(f'Max_Test_Recall: {Max_test_Recall:.1%}, Max_BLC_Test_Recall: {Max_BLC_Test_Recall:.1%}, Max_Tr_Acc: {Max_train_Acc:.1%}, Max_V_Acc: {Max_valid_Acc:.1%}, Max_Te_Acc: {Max_test_Acc:.1%} ')\n","    pbar.set_postfix_str(f'Train_Acc: {train_accuracy:.1%} , Valide_Acc: {valid_accuracy:.1%} , Test_Acc: {test_accuracy:.1%}')\n","\n","    new_row = pd.Series({'model_name':model_name,'hyper_param':p,'test_overall_metric':test_overall_metric,'test_recall':test_recall,'blc_test_recall':blc_test_recall,'valid_recall':valid_recall,'train_accuracy':train_accuracy,'valid_accuracy':valid_accuracy,'test_accuracy':test_accuracy,\n","                         'train_epoch_loss':train_epoch_loss,'train_epoch_acc':train_epoch_acc,'valid_epoch_loss':valid_epoch_loss,'valid_epoch_acc':valid_epoch_acc,'test_epoch_acc':test_epoch_acc,'test_epoch_loss':test_epoch_loss}, name='')\n","    Hyper_Details = Hyper_Details.append(new_row)\n","    m_details = pd.DataFrame()\n","    m_details = m_details.append(new_row)\n","\n","\n","\n","\n","    if save_results:\n","      # Hyper_Details.to_csv(G_path +'/Saved/Grid_Search_Results/' + export_name + date_hour+ '.csv')\n","      \n","      pickle.dump(m_details, open(G_path +'/08_Saved_Models_Outpus/Grid_Search_Results/' + model_name, 'wb'))\n","      pickle.dump(Hyper_Details, open(G_path +'/08_Saved_Models_Outpus/Grid_Search_Results/' + 'Grid_All_' + export_name + date_hour , 'wb'))\n","\n","    if save_models:\n","      pickle.dump(model_, open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  model_name, 'wb'))\n","\n","    Best_Grid_Model = pickle.load(open(G_path +'/08_Saved_Models_Outpus/Models/CNN_Grid_Search_Models/' +  'Best_Grid_Model' +str(i) +'_' + date_hour, 'rb'))\n","\n","Hyper_Details.sort_values('test_overall_metric', ascending=False,inplace=True)\n","best_param = Hyper_Details['hyper_param'][0]\n","best_params = Hyper_Details['hyper_param'][:4]\n","\n","# Grid_Details = Hyper_Details\n","\n","# Model_Grid = Model_Training_with_loader(**best_param,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=best_param)\n","# np.random.seed(0)\n","# random.seed(0)\n","# torch.manual_seed(0)\n","# Model_Grid.train()\n","# print('Max Avg. Recall on Test ====>> :')\n","# Grid_Details['test_recall'][:5], Grid_Details['hyper_param'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVnEzpk9FGTb"},"outputs":[],"source":["# model_ = Model_Grid\n","model_1 = Best_Grid_Model\n","X = input_data[2]\n","Y = labels[2]\n","# X = Evaluation_set[0]\n","# Y = Evaluation_set[1]\n","Y_pred = model_1.forward_noDrop(X).argmax(dim=1)\n","results = confusion_matrix(Y,Y_pred)\n","# plot_loss_accuracy(model_)\n","results"]},{"cell_type":"markdown","metadata":{"id":"zomymNkU4JNX"},"source":["# **Cross Validation: Repeat 5 times with 10 Kfold (overall 50 models)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qWnKjVHfeBE"},"outputs":[],"source":["from torch.nn.modules import loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLfqTs-54anr"},"outputs":[],"source":["# K_Folds_test = sk.model_selection.KFold(n_splits=10, shuffle=True)\n","K_Folds_valid = sk.model_selection.KFold(n_splits=10, shuffle=True)\n","dat_hour = datetime.now().strftime(\"%d_%b_%Y_%H_%M\")\n","print_epochs = False\n","\n","param1 = {\n","    'Net':CNN_Nets.Net8_b\n","    ,'Drop':0.2\n","    ,'LR':1.1e-3\n","    ,'batch_size':24\n","    ,'Momentum':0.77\n","    ,'epochs':100\n","    ,'patience':20\n","    ,'weight_decay':1e-3\n","    ,'loss_func' : \n","    loss.CrossEntropyLoss\n","    ,'opt_func' : \n","    torch.optim.SGD\n","}\n","\n","CV_data = torch.cat([input_data[0],input_data[1]],dim=0)\n","CV_label = torch.cat([labels[0],labels[1]],dim=0)\n","X_test = input_data[2]\n","Y_test = labels[2]\n","\n","\n","CV_Details = pd.DataFrame(columns=['hyper_param','train_recall_weighed','valid_recall_weighed','test_recall_weighed'\n","                                  ,'valid_recall_simple','test_recall_simple'\n","                                  ,'valid_specificity_weighed','test_specificity_weighed'\n","                                  ,'valid_specificity_simple','test_specificity_simple'])\n","\n","Max_train_Acc = 0  \n","Max_valid_Acc = 0 \n","Max_test_Acc = 0                  \n","\n","pbar1 = tqdm(range(5))\n","for i in pbar1:\n","    pbar2 = tqdm(enumerate(K_Folds_valid.split(CV_data)), total=K_Folds_valid.get_n_splits())\n","    for fold, (training_index, valid_index) in pbar2:\n","        X_train = CV_data[training_index]\n","        Y_train = CV_label[training_index]\n","        X_valid = CV_data[valid_index]\n","        Y_valid = CV_label[valid_index]\n","\n","        label_freq = np.bincount(Y_train)\n","        labels_weights = 1. / label_freq\n","        weights = labels_weights[Y_train]\n","        w_sampler = WeightedRandomSampler(weights, len(weights))\n","\n","        trainDataset = dset(X_train, Y_train)\n","        validDataset = dset(X_valid, Y_valid)\n","        testDataset = dset(X_test, Y_test)\n","\n","        Model_ = Model_Training_with_loader(**param1,w_sampler = w_sampler , trainDataset = trainDataset, validDataset = validDataset , X_test = X_test, Y_test = Y_test, print_epochs =print_epochs,hyper_params=param1)\n","        Model_.train()\n","\n","        model_ = Model_.model.eval().to('cpu')\n","            \n","        # Train\n","        train_recall_weighed = mean(Model_.Epochs_Train_Acc)\n","\n","        # Valid\n","        Y_pred = model_(X_valid).argmax(axis=1)\n","        Y = Y_valid\n","        valid_recall_weighed, valid_specificity_weighed, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=True)\n","        valid_recall_simple, valid_specificity_simple, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=False)\n","\n","        # Test\n","        Y_pred = model_(X_test).argmax(axis=1)\n","        Y = Y_test\n","        test_recall_weighed, test_specificity_weighed, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=True)\n","        test_recall_simple, test_specificity_simple, _ = recall_specificity_precision(Y,Y_pred,weighted_avg=False)\n","\n","\n","        if train_recall_weighed > Max_train_Acc: Max_train_Acc = train_recall_weighed\n","        if valid_recall_weighed > Max_valid_Acc: Max_valid_Acc = valid_recall_weighed\n","        if test_recall_weighed > Max_test_Acc: Max_test_Acc = test_recall_weighed\n","        new_row = pd.Series({'train_recall_weighed':train_recall_weighed,'valid_recall_weighed':valid_recall_weighed,'test_recall_weighed':test_recall_weighed\n","                                      ,'valid_recall_simple':valid_recall_simple,'test_recall_simple':test_recall_simple\n","                                      ,'valid_specificity_weighed':valid_specificity_weighed,'test_specificity_weighed':test_specificity_weighed\n","                                      ,'valid_specificity_simple':valid_specificity_simple,'test_specificity_simple':test_specificity_simple}, name='')\n","        CV_Details = CV_Details.append(new_row)\n","        pbar1.set_description(f'Max_train_Acc: {Max_train_Acc:.1%}, Max_valid_Acc: {Max_valid_Acc:.1%}, Max_test_Acc: {Max_test_Acc:.1%} ')\n","        pbar1.set_postfix_str(f'Train_Acc: {train_recall_weighed:.1%} , Valide_Acc: {valid_recall_weighed:.1%} , Test_Acc: {test_recall_weighed:.1%}')\n","\n","\n","        pickle.dump(CV_Details, open(G_path + '/08_Saved_Models_Outpus/Cross_Valid_Results/CV_280_MultiClass_'+ dat_hour,'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zKshOw9PaDt"},"outputs":[],"source":["col_recall = ['test_recall_weighed','valid_recall_weighed']\n","model_recalls = CV_Details[col_recall]\n","col_specificity = ['test_specificity_weighed','valid_specificity_weighed']\n","model_specificity = CV_Details[col_specificity]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYPZ9JOfPYIe"},"outputs":[],"source":["plt.subplots(figsize=(8,4))\n","sns.boxplot(data=model_recalls)\n","plt.title('Test performance of 100 Cross-Validation',fontsize = 18)\n","plt.xlabel(\"Classifiers\", fontsize = 14)\n","plt.ylabel(\"Accuracy\", fontsize = 14)\n","plt.xticks(fontsize=16, rotation=0)\n","plt.show()\n","\n","plt.subplots(figsize=(8,4))\n","sns.boxplot(data=model_specificity)\n","plt.title('Valid performance of 100 Cross-Validation',fontsize = 18)\n","plt.xlabel(\"Classifiers\", fontsize = 14)\n","plt.ylabel(\"Accuracy\", fontsize = 14)\n","plt.xticks(fontsize=16, rotation=0)\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"06_CNN_280x210_MultiClass.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}